{
    "docs": [
        {
            "location": "/", 
            "text": "Taskgrader\n\n\nThe taskgrader tool manages every step of grading a contest task, from the generation of test data to the grading of a solution output.\n\n\nIt allows for a wide variety of contest task types and languages to be evaluated, and is meant to be used both locally for tests and in contest evaluation settings.\n\n\nIt uses \nisolate\n as a sandbox to run solutions to limit execution time and memory, as well as their access to the environment.\n\n\nThis documentation covers:\n\n\n\n\nGetting started:\n\n\nInstallation\n\n\nBasic usage\n of the taskgrader\n\n\nHow to write tasks\n for use with the taskgrader\n\n\nInput JSON\n to send to the taskgrader\n\n\n\n\n\n\nReference:\n\n\ntaskSettings.json\n, the settings for auto-detection of the task components and parameters\n\n\ndefaultParams.json\n, the full description of the task for direct use by evaluation processes\n\n\n\n\n\n\nError messages\n and their meaning\n\n\nFurther information\n on the taskgrader internals", 
            "title": "Home"
        }, 
        {
            "location": "/#taskgrader", 
            "text": "The taskgrader tool manages every step of grading a contest task, from the generation of test data to the grading of a solution output.  It allows for a wide variety of contest task types and languages to be evaluated, and is meant to be used both locally for tests and in contest evaluation settings.  It uses  isolate  as a sandbox to run solutions to limit execution time and memory, as well as their access to the environment.  This documentation covers:   Getting started:  Installation  Basic usage  of the taskgrader  How to write tasks  for use with the taskgrader  Input JSON  to send to the taskgrader    Reference:  taskSettings.json , the settings for auto-detection of the task components and parameters  defaultParams.json , the full description of the task for direct use by evaluation processes    Error messages  and their meaning  Further information  on the taskgrader internals", 
            "title": "Taskgrader"
        }, 
        {
            "location": "/install/", 
            "text": "Installing the taskgrader\n\n\nDependencies\n\n\nOn many distributions, the required dependencies are already installed.\n\n\nOn \nDebian or Ubuntu\n, the recommended dependencies are:\n\n\napt-get install build-essential git python python3 sudo\n\n\n\nSome additional dependencies are required to support all features and languages, on Debian or Ubuntu:\n\n\napt-get install fp-compiler gcj-4.9 nodejs php5-cli\n\n\n\nOn \nFedora\n, the recommended dependencies are:\n\n\ndnf install @development-tools glibc-static libstdc++-static\n\n\n\nSome systems don't provide the \ngcj\n shortcut, in that case make a symlink to your version of \ngcj\n, such as:\n\n\nln -s /usr/bin/gcj-4.9 /usr/bin/gcj\n\n\n\nControl groups (for contest environments)\n\n\nIn a contest environment, you may want control groups enabled in your kernel:\n\n\napt-get install cgroup-tools\n\n\n\nOn some kernels, you might need to (re)activate the memory subsystem of control groups (on Debian, you can check whether the folder \n/sys/fs/cgroup/memory\n is present).\n\n\nYou can do this by using the \ncgroup_enable=memory\n kernel option. On many systems, you can do that by editing \n/etc/default/grub\n to add:\n\n\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory\"\n\n\n\nand then executing \nupdate-grub\n as root. Once enabled, set \nCFG_CONTROLGROUPS\n to \nTrue\n in \nconfig.py\n (after installation) to enable their usage within the taskgrader.\n\n\nSome more information can be found in the \nisolate man page\n.\n\n\nOn Windows\n\n\nYou can use the taskgrader in a Windows environment thanks to \nCygwin\n. Most features will work, except isolate, and some languages which are not supported under Cygwin.\n\n\nDownload and install Cygwin from \nits website\n. Install the following dependencies (you can reexecute Cygwin's setup.exe anytime to install more dependencies):\n\ngit python python3 gcc gcc-g++\n.\n\n\nYou can then follow the next steps normally.\n\n\nInstallation\n\n\nExecute \ninstall.sh\n in the taskgrader directory to install, as the user who will be running the taskgrader. It will help you install everything.\n\n\nThe installation of 'isolate' needs root access, and ability to have files owned by root with setuid on the current directory (doesn't work with remote folders such as NFS). If you cannot, you won't be able to use 'isolate', but the taskgrader will still work.\n\n\nIf needed, edit \nconfig.py\n to suit your needs; however default values will work for simple tests.\n\n\nTesting\n\n\nAfter configuration, you can test that the taskgrader is configured properly and is behaving as expected by running \ntests/test.py\n. By default, it will run all tests and give you a summary. Full usage instructions are given by \ntest.py -h\n.\n\n\nIf you didn't install dependencies for all languages, some tests will fail.\n\n\nUsage\n\n\nNow that the taskgrader is installed, you can use it as described in the \nBasic Usage\n section.\n\n\nFor convenience, you can add some taskgrader commands by adding the \npath/\n subfolder of this repository into your \nPATH\n environment variable. You can do so by adding to your shell profile file (for instance \n~/.bashrc\n if you use \nbash\n):\n\n\nexport PATH=\"/your/copy/of/taskgrader/path/:$PATH\"\n\n\n\nIt will allow a few commands to be used directly in your shell, such as \ntaskstarter\n or \ntaskgrader\n. Check the \npath/\n folder to see the full list of commands.", 
            "title": "Installing"
        }, 
        {
            "location": "/install/#installing-the-taskgrader", 
            "text": "", 
            "title": "Installing the taskgrader"
        }, 
        {
            "location": "/install/#dependencies", 
            "text": "On many distributions, the required dependencies are already installed.  On  Debian or Ubuntu , the recommended dependencies are:  apt-get install build-essential git python python3 sudo  Some additional dependencies are required to support all features and languages, on Debian or Ubuntu:  apt-get install fp-compiler gcj-4.9 nodejs php5-cli  On  Fedora , the recommended dependencies are:  dnf install @development-tools glibc-static libstdc++-static  Some systems don't provide the  gcj  shortcut, in that case make a symlink to your version of  gcj , such as:  ln -s /usr/bin/gcj-4.9 /usr/bin/gcj", 
            "title": "Dependencies"
        }, 
        {
            "location": "/install/#control-groups-for-contest-environments", 
            "text": "In a contest environment, you may want control groups enabled in your kernel:  apt-get install cgroup-tools  On some kernels, you might need to (re)activate the memory subsystem of control groups (on Debian, you can check whether the folder  /sys/fs/cgroup/memory  is present).  You can do this by using the  cgroup_enable=memory  kernel option. On many systems, you can do that by editing  /etc/default/grub  to add:  GRUB_CMDLINE_LINUX=\"cgroup_enable=memory\"  and then executing  update-grub  as root. Once enabled, set  CFG_CONTROLGROUPS  to  True  in  config.py  (after installation) to enable their usage within the taskgrader.  Some more information can be found in the  isolate man page .", 
            "title": "Control groups (for contest environments)"
        }, 
        {
            "location": "/install/#on-windows", 
            "text": "You can use the taskgrader in a Windows environment thanks to  Cygwin . Most features will work, except isolate, and some languages which are not supported under Cygwin.  Download and install Cygwin from  its website . Install the following dependencies (you can reexecute Cygwin's setup.exe anytime to install more dependencies): git python python3 gcc gcc-g++ .  You can then follow the next steps normally.", 
            "title": "On Windows"
        }, 
        {
            "location": "/install/#installation", 
            "text": "Execute  install.sh  in the taskgrader directory to install, as the user who will be running the taskgrader. It will help you install everything.  The installation of 'isolate' needs root access, and ability to have files owned by root with setuid on the current directory (doesn't work with remote folders such as NFS). If you cannot, you won't be able to use 'isolate', but the taskgrader will still work.  If needed, edit  config.py  to suit your needs; however default values will work for simple tests.", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#testing", 
            "text": "After configuration, you can test that the taskgrader is configured properly and is behaving as expected by running  tests/test.py . By default, it will run all tests and give you a summary. Full usage instructions are given by  test.py -h .  If you didn't install dependencies for all languages, some tests will fail.", 
            "title": "Testing"
        }, 
        {
            "location": "/install/#usage", 
            "text": "Now that the taskgrader is installed, you can use it as described in the  Basic Usage  section.  For convenience, you can add some taskgrader commands by adding the  path/  subfolder of this repository into your  PATH  environment variable. You can do so by adding to your shell profile file (for instance  ~/.bashrc  if you use  bash ):  export PATH=\"/your/copy/of/taskgrader/path/:$PATH\"  It will allow a few commands to be used directly in your shell, such as  taskstarter  or  taskgrader . Check the  path/  folder to see the full list of commands.", 
            "title": "Usage"
        }, 
        {
            "location": "/basicusage/", 
            "text": "Basic usage\n\n\nExecuting\n\n\nThe taskgrader itself can be executed with\n\n\npython taskgrader.py\n\n\n\nIt will wait for an input JSON on its standard input, then proceed to evaluation and then output the result JSON on standard output. Read \nschema_input.json\n and \nschema_output.json\n for a description of the expected formats. Various tools described later can help you write this input JSON.\n\n\nVerbosity options are available, use \ntaskgrader.py -h\n for more help.\n\n\nExample usage\n\n\nSome commands you can try:\n\n\n./taskgrader.py \n examples/testinput.json\n./taskgrader.py \n examples/testinput.json | tools/stdGrade/summarizeResults.py\n\n\n\nFirst command will execute the taskgrader on an example evaluation described by \nexamples/testinput.json\n; it will output the result JSON, which isn't very human-readable. The second command will pass that output JSON to \nsummarizeResults.py\n, a simple tool to show the results in a more human-readable way.\n\n\ntools/taskstarter/taskstarter.py init mynewtask\n\n\n\nThis command will start a new task in the folder \nmynewtask\n; use it if you want to write a task for use with the taskgrader. It will ask you a few questions to guide you through the various components of a task, and write a base task with some example files. The next section describes that in more detail.\n\n\ncd examples/taskMinimal ; ../../tools/taskstarter/taskstarter.py test\n\n\n\nA task may be tested as shown with this command. Here it will test the example task in folder \ntaskMinimal\n.\n\n\nMore details on usage can be found through this documentation.", 
            "title": "Basic usage"
        }, 
        {
            "location": "/basicusage/#basic-usage", 
            "text": "", 
            "title": "Basic usage"
        }, 
        {
            "location": "/basicusage/#executing", 
            "text": "The taskgrader itself can be executed with  python taskgrader.py  It will wait for an input JSON on its standard input, then proceed to evaluation and then output the result JSON on standard output. Read  schema_input.json  and  schema_output.json  for a description of the expected formats. Various tools described later can help you write this input JSON.  Verbosity options are available, use  taskgrader.py -h  for more help.", 
            "title": "Executing"
        }, 
        {
            "location": "/basicusage/#example-usage", 
            "text": "Some commands you can try:  ./taskgrader.py   examples/testinput.json\n./taskgrader.py   examples/testinput.json | tools/stdGrade/summarizeResults.py  First command will execute the taskgrader on an example evaluation described by  examples/testinput.json ; it will output the result JSON, which isn't very human-readable. The second command will pass that output JSON to  summarizeResults.py , a simple tool to show the results in a more human-readable way.  tools/taskstarter/taskstarter.py init mynewtask  This command will start a new task in the folder  mynewtask ; use it if you want to write a task for use with the taskgrader. It will ask you a few questions to guide you through the various components of a task, and write a base task with some example files. The next section describes that in more detail.  cd examples/taskMinimal ; ../../tools/taskstarter/taskstarter.py test  A task may be tested as shown with this command. Here it will test the example task in folder  taskMinimal .  More details on usage can be found through this documentation.", 
            "title": "Example usage"
        }, 
        {
            "location": "/writing/", 
            "text": "Getting started on writing a task\n\n\nUse \ntaskstarter.py init [taskpath]\n to interactively create a new task.\n\n\nA \"task\" is a set of programs and files representing the problem the solutions will be evaluated against:\n\n\n\n\nthe \ntest cases\n (input files and the associated expected output),\n\n\nthe \nlibraries\n the solutions can use\n\n\nan optional \ngenerator\n which generates these two types of files\n\n\na \nsanitizer\n, validating the input files by checking they are in the required format\n\n\nthe \nchecker\n, grading each solution's output\n\n\n\n\nFirst steps\n\n\nThe script \ntools/taskstarter/taskstarter.py\n can assist with writing a task; use \ntaskstarter.py help\n to see the available commands.\n\n\nYou might start a new task interactively by executing \ntaskstarter.py init\n.\n\n\nAlternatively, you can also copy one of the \nexample tasks\n and use it as a base for your task.\n\n\nTesting tasks\n\n\nTasks can be tested with:\n\n\n\n\ntaskstarter.py test\n, which will use the tool \ngenJson\n to prepare the task for usage (read about \ndefaultParams.json\n file below for more information) and test it for valid compilation, and test that the \"correct solutions\" get the expected grades.\n\n\ntaskstarter.py testsol [SOLUTION.c]\n, which if the task is valid, will test \nSOLUTION.c\n against the task. It is meant for quick solution testing; it uses the \nstdGrade\n tool.\n\n\ntaskstarter.py testsol -v [SOLUTION.c]\n, which will give a more verbose output, showing all the information given by the taskgrader. It can help pinpointing errors.\n\n\n\n\nRemote testing\n\n\nIf you have access to a \ngraderqueue\n instance, you can evaluate your task through it (for instance, to evaluate on contest servers).\n\n\nOnce you configured the remoteGrader in the file \ntools/remoteGrader/config.py\n with the URL, login and password to a graderqueue, you can use taskstarter to test remotely.\n\n\nOnce you have tested your task locally, you can use:\n\n\ntaskstarter.py remotetest [SOLUTION.c]\n\n\n\nto test your task and \nSOLUTION.c\n with a remote server. It will behave as the \ntestsol\n command, except using a remote server instead of the local taskgrader. It will send a \"full input JSON\", which means that all task files will be contained in the JSON file and the remote server doesn't need any file from your task to evaluate.\n\n\nIf your task is saved/synchronized on the remote server, and the \nROOT_PATH\n is configured accordingly (contact your graderqueue/graderserver administrator to know how to configure it), you can test your task with a \"simple JSON\" with the following command:\n\n\ntaskstarter.py remotetest -s [SOLUTION.c]\n\n\n\nIt will send a simple solution with only your solution; the remote server will then read the task files locally for the evaluation. It's thus important that the remote server has the most recent files for the evaluation; if the task are locally and remotely on a SVN repository, taskstarter will check the task has been committed and send the corresponding revision number to the remote server for it to make sure it's on the latest version.\n\n\nUsing tasks\n\n\nThe tool \ngenJson\n, automatically called when using \ntaskstarter.py test\n, prepares the task by writing its parameters into a \ndefaultParams.json\n file. It contains all the required information to evaluate solutions against the task, and can be used by evaluation platforms directly to reference the task. The tool \nstdGrade\n will use this file to quickly evaluate solutions.\n\n\nExamples\n\n\nCheck the \nexamples page\n for a description of the examples in the taskgrader repository. These examples can be easily used as a base for your task.", 
            "title": "Writing tasks"
        }, 
        {
            "location": "/writing/#getting-started-on-writing-a-task", 
            "text": "Use  taskstarter.py init [taskpath]  to interactively create a new task.  A \"task\" is a set of programs and files representing the problem the solutions will be evaluated against:   the  test cases  (input files and the associated expected output),  the  libraries  the solutions can use  an optional  generator  which generates these two types of files  a  sanitizer , validating the input files by checking they are in the required format  the  checker , grading each solution's output", 
            "title": "Getting started on writing a task"
        }, 
        {
            "location": "/writing/#first-steps", 
            "text": "The script  tools/taskstarter/taskstarter.py  can assist with writing a task; use  taskstarter.py help  to see the available commands.  You might start a new task interactively by executing  taskstarter.py init .  Alternatively, you can also copy one of the  example tasks  and use it as a base for your task.", 
            "title": "First steps"
        }, 
        {
            "location": "/writing/#testing-tasks", 
            "text": "Tasks can be tested with:   taskstarter.py test , which will use the tool  genJson  to prepare the task for usage (read about  defaultParams.json  file below for more information) and test it for valid compilation, and test that the \"correct solutions\" get the expected grades.  taskstarter.py testsol [SOLUTION.c] , which if the task is valid, will test  SOLUTION.c  against the task. It is meant for quick solution testing; it uses the  stdGrade  tool.  taskstarter.py testsol -v [SOLUTION.c] , which will give a more verbose output, showing all the information given by the taskgrader. It can help pinpointing errors.", 
            "title": "Testing tasks"
        }, 
        {
            "location": "/writing/#remote-testing", 
            "text": "If you have access to a  graderqueue  instance, you can evaluate your task through it (for instance, to evaluate on contest servers).  Once you configured the remoteGrader in the file  tools/remoteGrader/config.py  with the URL, login and password to a graderqueue, you can use taskstarter to test remotely.  Once you have tested your task locally, you can use:  taskstarter.py remotetest [SOLUTION.c]  to test your task and  SOLUTION.c  with a remote server. It will behave as the  testsol  command, except using a remote server instead of the local taskgrader. It will send a \"full input JSON\", which means that all task files will be contained in the JSON file and the remote server doesn't need any file from your task to evaluate.  If your task is saved/synchronized on the remote server, and the  ROOT_PATH  is configured accordingly (contact your graderqueue/graderserver administrator to know how to configure it), you can test your task with a \"simple JSON\" with the following command:  taskstarter.py remotetest -s [SOLUTION.c]  It will send a simple solution with only your solution; the remote server will then read the task files locally for the evaluation. It's thus important that the remote server has the most recent files for the evaluation; if the task are locally and remotely on a SVN repository, taskstarter will check the task has been committed and send the corresponding revision number to the remote server for it to make sure it's on the latest version.", 
            "title": "Remote testing"
        }, 
        {
            "location": "/writing/#using-tasks", 
            "text": "The tool  genJson , automatically called when using  taskstarter.py test , prepares the task by writing its parameters into a  defaultParams.json  file. It contains all the required information to evaluate solutions against the task, and can be used by evaluation platforms directly to reference the task. The tool  stdGrade  will use this file to quickly evaluate solutions.", 
            "title": "Using tasks"
        }, 
        {
            "location": "/writing/#examples", 
            "text": "Check the  examples page  for a description of the examples in the taskgrader repository. These examples can be easily used as a base for your task.", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/", 
            "text": "Examples index\n\n\nThe taskgrader comes with various examples depicting features of the taskgrader. Each of these examples is described in detail in this page.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntaskCChecker\n\n\nCheck the output of the solution compilation\n\n\n\n\n\n\ntaskGenerator\n\n\nGenerate test cases with a \ngenerator\n script\n\n\n\n\n\n\ntaskInputs\n\n\nHave the solution read a specific file then output in another file, instead of stdin/stdout\n\n\n\n\n\n\ntaskLibs\n\n\nAdd libraries for usage by the solutions\n\n\n\n\n\n\ntaskMaths\n\n\nCount the number of math operations performed\n\n\n\n\n\n\ntaskMinimal\n\n\nMinimal task with only test cases and expected outputs\n\n\n\n\n\n\ntaskRunner\n\n\nRun a Python function defined by the solution instead of a full program\n\n\n\n\n\n\ntaskSets\n\n\nSeparate multiple sets of tests\n\n\n\n\n\n\ntaskSolchecker\n\n\nRead the solution source (to check for length)\n\n\n\n\n\n\ntaskSpecialChars\n\n\nUTF-8 inputs and outputs test\n\n\n\n\n\n\ntaskTestchecker\n\n\nSolution is not a program but a set of test cases\n\n\n\n\n\n\ntaskTools\n\n\nCustom sanitizer and checker\n\n\n\n\n\n\ntaskTurtle\n\n\nCheck a turtle program and output its display\n\n\n\n\n\n\n\n\nBasic examples\n\n\nHere are some basic examples based around a simple problem: the program is given a number as input, and must output the double of the number. These example tasks can be found in the \nexamples\n folder.\n\n\nMinimal example: only test cases (taskMinimal)\n\n\nA task can be just test cases and their corresponding expected outputs. The task can be built and tested like this:\n\n\n\n\nWe start our task in a folder with \ntaskstarter.py init\n (answering 'no' to all questions); it will give us a base task structure we can use as reference\n\n\nWe put the test cases input files \ntest1.in\n and \ntest2.in\n in the subfolder \ntests/files/\n\n\nWe put in the same subfolder \ntest1.out\n and \ntest2.out\n, the correct output of these test cases\n\n\nWe can write a valid solution, for instance \nsol-ok-c.c\n in this example\n\n\nWe can test this solution with \ntaskstarter.py testsol tests/gen/sol-ok-c.c\n, it will say the solution got a grade of 100 for each test\n\n\n\n\nWhen the task is only test cases, the tools will use default programs as sanitizer and checker. The solution will get a perfect grade of 100 if its output is the expected output (\ntest1.out\n for the test case \ntest1.in\n, ...), or the minimum grade of 0 if its output is different.\n\n\nThis example can be found in the \nexamples/taskMinimal\n folder.\n\n\nAdding correct solutions to our task\n\n\nOnce our task is almost ready, we can add \"correct solutions\": they are known solutions which have known results, such as always giving a good answer or always giving a bad answer.\n\n\nIt allows to test against regressions, that is to say that after modifications, that the task still gives the right grade to our known solutions.\n\n\nUsing again our example task 1, we can add correct solutions like this:\n\n\ntaskstarter.py addsol -g 100 -l c tests/gen/sol-ok-c.c\n\n\n\nIt means that each time we'll test the task, the solution \nsol-ok-c.c\n will be automatically evaluated against the task, and \n-g 100\n means we expect the solution to get a perfect grade of 100 each time (and having a final average grade of 100).\n\n\nWe can also add an invalid solution, that is to say, a solution who will get the minimum grade of 0 each time. In this example, we have such a solution, \nsol-bad-c.c\n. We add it as a \"bad solution\" like this:\n\n\ntaskstarter.py addsol -g 0 -l c tests/gen/sol-bad-c.c\n\n\n\nwhich means that we expect it to get a grade of 0 on each test (hence a final expected average grade of 0).\n\n\nWe can finally test the task by executing, in the task folder,\n\n\ntaskstarter.py test\n\n\n\nin the task folder. It will output:\n\n\nTest successful on 2 correctSolutions with up to 2 test cases.\n\n\n\nwhich means the task successfully compiled, that the evaluation of the good solution \nsol-ok-c.c\n got a perfect grade of 100 each time, and that the evaluation of the bad solution \nsol-bad-c.c\n got a bad grade of 0 each time.\n\n\nTools example: adding a sanitizer and a checker (taskTools)\n\n\nIt's generally recommended to use a sanitizer and a checker: the sanitizer will ensure only valid input test files are given to solutions, especially in cases where contestants are allowed to use their own test files as examples; the checker can give a more precise grade to solutions, and also handle cases where the solution output is not in the exact format expected.\n\n\nWe add a sanitizer and a checker to our test like this:\n\n\n\n\nWe use the task made in the last example\n\n\nWe write a script \nsanitizer.sh\n which takes as input the test case, and sets its exit code to 0 if it's valid, 1 if it's not\n\n\nWe write a script \nchecker.sh\n which takes three arguments, \ntest.in\n the test case input, \ntest.solout\n the solution output, \ntest.out\n the reference (expected) output; and gives a grade to the solution based on its output\n\n\nWe add the sanitizer and the checker to the task with \ntaskstarter.py add checker tests/gen/checker.sh\n and \ntaskstarter.py add sanitizer tests/gen/sanitizer.sh\n\n\nFinally, we test the task with \ntaskstarter.py test\n. It will tell us whether our programs were detected, compiled and executed successfully, and whether the \"correct solution\" we defined passed the test and got the expected grade of 100.\n\n\n\n\nThe sanitizer and the checker can be written in any language supported by the taskgrader.\n\n\nThis example can be found in the \nexamples/taskTools\n folder.\n\n\nLibs example: adding libraries (taskLibs)\n\n\nThis example is a task with libraries intended for usage by solutions. It's pretty simple to add libraries:\n\n\n\n\nWe write a library \nlib.h\n intended for usage with solution in the C language\n\n\nWe put it in \ntests/files/lib/c/lib.h\n\n\nFinally, we test the task with \ntaskstarter.py test\n.\n\n\n\n\nAs long as the libraries are in the right folder (by default, \ntests/files/lib/[language]/[library.ext]\n), they will be automatically detected by \ngenJson\n and added to the default dependencies for that language. All solutions of that language will have the library available for importation without any configuration necessry.\n\n\nThis example can be found in the \nexamples/taskLibs\n folder.\n\n\nGenerator example: adding a generator (taskGenerator)\n\n\nIt can be handy to use a generator to generate the test cases and/or libraries for the task, instead of writing them all by hand.\n\n\nWe add a generator like this:\n\n\n\n\nWe use the task made in the last example\n\n\nWe write a script \ngen.sh\n which generates the files upon execution; it must be a shell script, and it must write the files following the same tree as the \nfiles\n folder\n\n\nWe add the generator to the task with \ntaskstarter.py add generator tests/gen/gen.sh\n\n\nFinally, we test the task with \ntaskstarter.py test\n.\n\n\n\n\nThe generator is handy when a large number of test cases must be generated, and also for complex tasks where the expected output can take a long time to be computed, and thus needs to be precomputed.\n\n\nNote that the auto-test with \ntaskstarter.py test\n will show:\n\n\nTest successful on 2 correctSolutions with up to 4 test cases.\n\n\n\nwhich means the taskgrader successfully found all 4 test cases, the 2 test cases given directly in the task folder, and the 2 test cases generated by our script \ngen.sh\n.\n\n\nThis example can be found in the \nexamples/taskGenerator\n folder.\n\n\nSpecial characters example: use UTF-8\n\n\nThis example is simply a test of inputs and outputs in UTF-8. You can use it to test your installation, or to check your own programs support UTF-8 encoding.\n\n\nThis example can be found in the \nexamples/taskSpecialCharacters\n folder.\n\n\nTest sets example: having multiple executions with different test sets (taskSets)\n\n\nIn some tasks, we can have test sets which are testing different aspects of the task. We can set the default to be evaluating each set separately, for instance when we want to have an average grade for each set of tests and not all tests at once.\n\n\nIt can be done by editing the \ntaskSettings.json\n file and adding a \ndefaultEvaluationExecutions\n key describing each test set:\n\n\n\"defaultEvaluationExecutions\": [\n    {\"id\": \"test-positive\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"testpos1.in\", \"testpos2.in\", \"testpos3.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    },\n    {\"id\": \"test-negative\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"testneg1.in\", \"testneg2.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    },\n    {\"id\": \"test-all\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"test*.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    }]\n}\n\n\n\nHere we have three test sets, \ntest-positive\n, \ntest-negative\n and \ntest-all\n, the first one using only positive tests, the second one using only negative tests, and the last one using all test cases.\n\n\nNote that if the cache is enabled, the solution will not be executed again for the \ntest-all\n set as all test cases have already been evaluated. Instead, the taskgrader will just fetch back the results from the cache.\n\n\nThis example can be found in the \nexamples/taskSets\n folder.\n\n\nMore complex task writing\n\n\nMore complex tasks can be written for usage with the taskgrader. The \ntaskstarter\n tool is meant for simple tasks, you need to edit files manually for these examples. Here is an example, but read the rest of this documentation for more information.\n\n\nRunner example: solution skeleton (taskRunner)\n\n\nSometimes, the \"solution\" to be evaluated is not the file to be executed, but a library or a test file.\n\n\nIn this example, we have \nrunner.py\n calling the function \nmin3nombres\n from the user-sent python file. The \"solution\" is hence a library, and the actual script executed is \nrunner.py\n using this library.\n\n\nIn order to be able to evaluate solutions against this task, we add, in \ntaskSettings.json\n, the key \ndefaultEvaluationSolutions\n. This key will get copied directly to \ndefaultParams.json\n, where it will indicate the skeleton of what needs to be executed by the taskgrader. Some more information can be found in the \ndefaultParams.json\n reference\n. Here this key contains:\n\n\n\"defaultEvaluationSolutions\": [{\n    \"id\": \"@solutionId\",\n    \"compilationDescr\": {\n        \"language\": \"python\",\n        \"files\": [{\"name\": \"runner.py\",\n                   \"path\": \"$TASK_PATH/tests/gen/runner.py\"}],\n        \"dependencies\": [{\"name\": \"solution.py\",\n                          \"path\": \"@solutionPath\",\n                          \"content\": \"@solutionContent\"}]\n        },\n    \"compilationExecution\": \"@defaultSolutionCompParams\"}]\n\n\n\nThis example means to use \nrunner.py\n as script to execute (hence in \"files\"), and to give the solution as dependency, stored in \nsolution.py\n (whichever the original solution filename is, it will be renamed to \nsolution.py\n).\n\n\nIn this key, we indicate what needs to be executed, and the values \n'@solutionFilename', '@solutionLanguage', '@solutionDependencies', '@solutionPath', '@solutionContent'\n will be replaced with the solution's name, language, dependencies, and its path or content. In this example task, \nrunner.py\n is defined as the main program to execute, and our solution is passed as dependency of this program, and automatically named \nsolution.py\n to be then imported by \nrunner.py\n.\n\n\nYou can test the task by running, from the task folder:\n\n\n\n\ntaskstarter.py test\n for the normal test\n\n\ntaskstarter.py testsol tests/gen/sol-ok-py.py\n for a good solution\n\n\ntaskstarter.py testsol tests/gen/sol-bad-py.py\n for a bad solution\n\n\n\n\nThe testing tool \nstdGrade\n will automatically understand how to evaluate these solutions.\n\n\nThis example can be found in the \nexamples/taskRunner\n folder.\n\n\nAnother runner example: counting the number of mathematical operations\n\n\nIn this example, we count the number of mathematical operations to make sure the solution doesn't do too many. To do this, we use a runner which modify the execution environment of the solution : it will replace the \nint\n function by one returning a \nNumber\n class. This class is a custom class allowing us to count the number of operations and block some operations from being used (in this example, only addition and substraction are allowed, multiplication and division aren't).\n\n\nThe runner outputs the statistics at the end of the script, which are then read back by the checker.\n\n\nThis example can be found in the \nexamples/taskMaths\n folder.\n\n\nSolution checker example: reading the solution source (taskSolchecker)\n\n\nIn this task, the checker grades the solution source code.\n\n\nFor this example, the checker counts the number of characters of the solution (excluding comment lines and whitespaces), and gives a lower grade if the solution source is too long. (Of course it gives a grade of 0 if the solution doesn't output the correct answer.)\n\n\nIn order to have the checker be able to read the solution source, we use the \naddFiles\n key of its \nrunExecution\n params; this key allows to add any file in the working folder during an execution. Here, it adds to the checker execution an access to the solution source, stored in file \nsolution\n. The checker can then read this file, along with the test case files, to accordingly grade the solution.\n\n\nHere, we modify the key \ndefaultChecker\n in order to add the file, like this:\n\n\n\"defaultChecker\": {\n    \"compilationDescr\": {\n        \"language\": \"python2\",\n        \"files\": [{\n            \"name\": \"checker.py\",\n            \"path\": \"$TASK_PATH/tests/gen/checker.py\"\n            }],\n        \"dependencies\": []},\n    \"compilationExecution\": \"@defaultToolCompParams\",\n    \"runExecution\": {\n        \"memoryLimitKb\": 131072,\n        \"timeLimitMs\": 60000,\n        \"useCache\": true,\n        \"stdoutTruncateKb\": -1,\n        \"stderrTruncateKb\": -1,\n        \"addFiles\": [{\n            \"name\": \"solution\",\n            \"path\": \"@solutionPath\",\n            \"content\": \"@solutionContent\"\n            }],\n        \"getFiles\": []}}\n}\n\n\n\nYou can test the task by running, from the task folder:\n\n\n\n\ntaskstarter.py test\n for the normal test, which will check each solution has the expected grade\n\n\ntaskstarter.py testsol tests/gen/sol-ok-c.c\n for a good and short solution getting the perfect grade of 100 (right answer, less than 60 characters)\n\n\ntaskstarter.py testsol tests/gen/sol-long-c.c\n for a solution too long getting a grade of 93 (right answer, more than 60 characters)\n\n\ntaskstarter.py testsol tests/gen/sol-bad-c.c\n for a bad solution getting a grade of 0 (wrong answer)\n\n\n\n\nThis example can be found in the \nexamples/taskSolchecker\n folder.\n\n\nCompilation checker example: reading the compiler output (taskCChecker)\n\n\nIn this task, the checker reads the compiler output. It then, as an example, displays it on his grading output; a more detailed checker could though read the compiler output to check for specific errors.\n\n\nThis is done by using again the \naddFiles\n key as done in the previous example, and adding the compiler output in the files to give to the checker:\n\n\n\"addFiles\": [\n  {\n    \"name\": \"compilationStdout\",\n    \"path\": \"$BUILD_PATH/solutions/solution/stdout\"\n  }, {\n    \"name\": \"compilationStderr\",\n    \"path\": \"$BUILD_PATH/solutions/solution/stderr\"\n  }]\n\n\n\nThis will give the checker access to the compilation output through the files \ncompilationStdout\n and \ncompilationStderr\n.\n\n\nThis example can be found in the \nexamples/taskCChecker\n folder.\n\n\nSpecific input/output files examples (taskInputs)\n\n\nIn this task, the solution has to read from a file named \ninput\n, and write its answer to the file \noutput\n.\n\n\nThis is done, like previous examples, by using the \naddFiles\n key, to add for each test a new file \ninput\n containing the new input we want to give to the solution.\n\n\nIn the \ntaskSettings.json\n file, we use the \ndefaultEvaluationExecutions\n to specify each test: set the contents of the \ninput\n file, and specify we want to capture the file \noutput\n (optional, allows to get its contents in the output JSON but unneeded for the checker to have access to this file).\n\n\nThis example can be found in the \nexamples/taskInputs\n folder.\n\n\nTurtle example: using and saving graphics (taskTurtle)\n\n\nIn this task, the solution is a \nturtle\n script; here, its task is to simply to go to a specified position (but more advanced criterias can be written).\n\n\nThis task is complex for two reasons:\n\n\n\n\nthe base turtle library needs a graphic window to work (unavailable on an evaluation server), and we still need to \"isolate\" the solution execution\n\n\nwe save the resulting image as a PNG file\n\n\n\n\nRequirements\n\n\nTo work, this task needs:\n\n\n\n\nXvfb, \"X virtual framebuffer\", a X server without display (to make turtle show graphics and then save them)\n\n\nPIL, \"Python Imaging Library\", a library to handle images\n\n\nTkinter, used by the turtle for its graphics\n\n\nGhostscript, for PIL to read the saved output from Tk\n\n\n\n\nOn Debian, they can be installed with the following command:\n\n\napt-get install xvfb python-pil python-tk ghostscript\n\n\n\nThe task also needs the \nchecker.sh\n to be whitelisted for use outside of isolate; this is done by adding its path in \nCFG_NOISOLATE\n in taskgrader's \nconfig.py\n file, for instance:\n\n\nCFG_NOISOLATE = [\"/path/to/taskgrader/examples/taskTurtle/tests/gen/checker.sh\"]\n\n\n\n(It must be an absolute path, as returned by Python's \nos.path.abspath\n.)\n\n\nHow it works\n\n\nThis task works by saving all turtle commands executed by the solution, and then replaying them to generate the resulting PNG, and also to evaluate the criteria of task completion.\n\n\nrunner.py\n\n\nThe execution of solutions is wrapped in a \nrunner.py\n execution.\n\n\nThe runner of this task defines a class, \nLoggedTurtle\n, which logs all the turtle commands executed while inhibiting their graphics. It also modifies the solution source code to change all instances of the normal \nTurtle\n class to this \nLoggedTurtle\n class; and then executes the solution while logging all commands. It finally prints the full JSON-encoded log, whose entries contain the following elements:\n\n\n\n\nID number for the turtle (to allow multiple turtles)\n\n\nComponent, either 'nav' for the navigator, 'turtle' for other functions\n\n\nName of turtle's function called\n\n\nargs and kwargs of the call\n\n\n\n\nSee \nturtleToPng.py\n or \nchecker.py\n for examples on how to use this log.\n\n\nchecker.sh\n\n\nThis script has two sub-components:\n\n\n\n\nturtleToPng.py\n is a script generating the base64-encoded PNG image resulting from the execution of the turtle\n\n\nchecker.py\n is the actual checker, using the turtle log to evaluate the criteria and grade the solution\n\n\n\n\nAs said above, this script must be whitelisted for executiong outside of isolate by the taskgrader. It will allow the script to run Xvfb and \nturtleToPng.py\n outside of isolate, as X displays cannot run inside isolate.\n\n\nIt will then use the taskgrader tool \ntools/isolate-run.py\n to run the actual checker \nchecker.py\n inside isolate.\n\n\nUsage\n\n\nAs usual, you can test the task by running, from the task folder:\n\n\n\n\ntaskstarter.py test\n for the normal test, which will check each solution has the expected grade\n\n\ntaskstarter.py testsol tests/gen/sol-ok-py.py\n for a good solution\n\n\ntaskstarter.py testsol tests/gen/sol-bad-py.py\n for a bad solution\n\n\n\n\nIf you grade a solution against the task, the output JSON will contain a file \nturtle_png.b64\n, which is the base64-encoded PNG image resulting from each execution of the solution against a test case. It is in the \nfiles\n key of each \nchecker\n report (in \n/executions[idx]/testReports[idx]\n). The file \nview.html\n in the task folder contains an example JavaScript to display it in a browser.\n\n\nThis example can be found in the \nexamples/taskTurtle\n folder.", 
            "title": "Task examples"
        }, 
        {
            "location": "/examples/#examples-index", 
            "text": "The taskgrader comes with various examples depicting features of the taskgrader. Each of these examples is described in detail in this page.     Name  Description      taskCChecker  Check the output of the solution compilation    taskGenerator  Generate test cases with a  generator  script    taskInputs  Have the solution read a specific file then output in another file, instead of stdin/stdout    taskLibs  Add libraries for usage by the solutions    taskMaths  Count the number of math operations performed    taskMinimal  Minimal task with only test cases and expected outputs    taskRunner  Run a Python function defined by the solution instead of a full program    taskSets  Separate multiple sets of tests    taskSolchecker  Read the solution source (to check for length)    taskSpecialChars  UTF-8 inputs and outputs test    taskTestchecker  Solution is not a program but a set of test cases    taskTools  Custom sanitizer and checker    taskTurtle  Check a turtle program and output its display", 
            "title": "Examples index"
        }, 
        {
            "location": "/examples/#basic-examples", 
            "text": "Here are some basic examples based around a simple problem: the program is given a number as input, and must output the double of the number. These example tasks can be found in the  examples  folder.", 
            "title": "Basic examples"
        }, 
        {
            "location": "/examples/#minimal-example-only-test-cases-taskminimal", 
            "text": "A task can be just test cases and their corresponding expected outputs. The task can be built and tested like this:   We start our task in a folder with  taskstarter.py init  (answering 'no' to all questions); it will give us a base task structure we can use as reference  We put the test cases input files  test1.in  and  test2.in  in the subfolder  tests/files/  We put in the same subfolder  test1.out  and  test2.out , the correct output of these test cases  We can write a valid solution, for instance  sol-ok-c.c  in this example  We can test this solution with  taskstarter.py testsol tests/gen/sol-ok-c.c , it will say the solution got a grade of 100 for each test   When the task is only test cases, the tools will use default programs as sanitizer and checker. The solution will get a perfect grade of 100 if its output is the expected output ( test1.out  for the test case  test1.in , ...), or the minimum grade of 0 if its output is different.  This example can be found in the  examples/taskMinimal  folder.", 
            "title": "Minimal example: only test cases (taskMinimal)"
        }, 
        {
            "location": "/examples/#adding-correct-solutions-to-our-task", 
            "text": "Once our task is almost ready, we can add \"correct solutions\": they are known solutions which have known results, such as always giving a good answer or always giving a bad answer.  It allows to test against regressions, that is to say that after modifications, that the task still gives the right grade to our known solutions.  Using again our example task 1, we can add correct solutions like this:  taskstarter.py addsol -g 100 -l c tests/gen/sol-ok-c.c  It means that each time we'll test the task, the solution  sol-ok-c.c  will be automatically evaluated against the task, and  -g 100  means we expect the solution to get a perfect grade of 100 each time (and having a final average grade of 100).  We can also add an invalid solution, that is to say, a solution who will get the minimum grade of 0 each time. In this example, we have such a solution,  sol-bad-c.c . We add it as a \"bad solution\" like this:  taskstarter.py addsol -g 0 -l c tests/gen/sol-bad-c.c  which means that we expect it to get a grade of 0 on each test (hence a final expected average grade of 0).  We can finally test the task by executing, in the task folder,  taskstarter.py test  in the task folder. It will output:  Test successful on 2 correctSolutions with up to 2 test cases.  which means the task successfully compiled, that the evaluation of the good solution  sol-ok-c.c  got a perfect grade of 100 each time, and that the evaluation of the bad solution  sol-bad-c.c  got a bad grade of 0 each time.", 
            "title": "Adding correct solutions to our task"
        }, 
        {
            "location": "/examples/#tools-example-adding-a-sanitizer-and-a-checker-tasktools", 
            "text": "It's generally recommended to use a sanitizer and a checker: the sanitizer will ensure only valid input test files are given to solutions, especially in cases where contestants are allowed to use their own test files as examples; the checker can give a more precise grade to solutions, and also handle cases where the solution output is not in the exact format expected.  We add a sanitizer and a checker to our test like this:   We use the task made in the last example  We write a script  sanitizer.sh  which takes as input the test case, and sets its exit code to 0 if it's valid, 1 if it's not  We write a script  checker.sh  which takes three arguments,  test.in  the test case input,  test.solout  the solution output,  test.out  the reference (expected) output; and gives a grade to the solution based on its output  We add the sanitizer and the checker to the task with  taskstarter.py add checker tests/gen/checker.sh  and  taskstarter.py add sanitizer tests/gen/sanitizer.sh  Finally, we test the task with  taskstarter.py test . It will tell us whether our programs were detected, compiled and executed successfully, and whether the \"correct solution\" we defined passed the test and got the expected grade of 100.   The sanitizer and the checker can be written in any language supported by the taskgrader.  This example can be found in the  examples/taskTools  folder.", 
            "title": "Tools example: adding a sanitizer and a checker (taskTools)"
        }, 
        {
            "location": "/examples/#libs-example-adding-libraries-tasklibs", 
            "text": "This example is a task with libraries intended for usage by solutions. It's pretty simple to add libraries:   We write a library  lib.h  intended for usage with solution in the C language  We put it in  tests/files/lib/c/lib.h  Finally, we test the task with  taskstarter.py test .   As long as the libraries are in the right folder (by default,  tests/files/lib/[language]/[library.ext] ), they will be automatically detected by  genJson  and added to the default dependencies for that language. All solutions of that language will have the library available for importation without any configuration necessry.  This example can be found in the  examples/taskLibs  folder.", 
            "title": "Libs example: adding libraries (taskLibs)"
        }, 
        {
            "location": "/examples/#generator-example-adding-a-generator-taskgenerator", 
            "text": "It can be handy to use a generator to generate the test cases and/or libraries for the task, instead of writing them all by hand.  We add a generator like this:   We use the task made in the last example  We write a script  gen.sh  which generates the files upon execution; it must be a shell script, and it must write the files following the same tree as the  files  folder  We add the generator to the task with  taskstarter.py add generator tests/gen/gen.sh  Finally, we test the task with  taskstarter.py test .   The generator is handy when a large number of test cases must be generated, and also for complex tasks where the expected output can take a long time to be computed, and thus needs to be precomputed.  Note that the auto-test with  taskstarter.py test  will show:  Test successful on 2 correctSolutions with up to 4 test cases.  which means the taskgrader successfully found all 4 test cases, the 2 test cases given directly in the task folder, and the 2 test cases generated by our script  gen.sh .  This example can be found in the  examples/taskGenerator  folder.", 
            "title": "Generator example: adding a generator (taskGenerator)"
        }, 
        {
            "location": "/examples/#special-characters-example-use-utf-8", 
            "text": "This example is simply a test of inputs and outputs in UTF-8. You can use it to test your installation, or to check your own programs support UTF-8 encoding.  This example can be found in the  examples/taskSpecialCharacters  folder.", 
            "title": "Special characters example: use UTF-8"
        }, 
        {
            "location": "/examples/#test-sets-example-having-multiple-executions-with-different-test-sets-tasksets", 
            "text": "In some tasks, we can have test sets which are testing different aspects of the task. We can set the default to be evaluating each set separately, for instance when we want to have an average grade for each set of tests and not all tests at once.  It can be done by editing the  taskSettings.json  file and adding a  defaultEvaluationExecutions  key describing each test set:  \"defaultEvaluationExecutions\": [\n    {\"id\": \"test-positive\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"testpos1.in\", \"testpos2.in\", \"testpos3.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    },\n    {\"id\": \"test-negative\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"testneg1.in\", \"testneg2.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    },\n    {\"id\": \"test-all\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": [\"test*.in\"],\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    }]\n}  Here we have three test sets,  test-positive ,  test-negative  and  test-all , the first one using only positive tests, the second one using only negative tests, and the last one using all test cases.  Note that if the cache is enabled, the solution will not be executed again for the  test-all  set as all test cases have already been evaluated. Instead, the taskgrader will just fetch back the results from the cache.  This example can be found in the  examples/taskSets  folder.", 
            "title": "Test sets example: having multiple executions with different test sets (taskSets)"
        }, 
        {
            "location": "/examples/#more-complex-task-writing", 
            "text": "More complex tasks can be written for usage with the taskgrader. The  taskstarter  tool is meant for simple tasks, you need to edit files manually for these examples. Here is an example, but read the rest of this documentation for more information.", 
            "title": "More complex task writing"
        }, 
        {
            "location": "/examples/#runner-example-solution-skeleton-taskrunner", 
            "text": "Sometimes, the \"solution\" to be evaluated is not the file to be executed, but a library or a test file.  In this example, we have  runner.py  calling the function  min3nombres  from the user-sent python file. The \"solution\" is hence a library, and the actual script executed is  runner.py  using this library.  In order to be able to evaluate solutions against this task, we add, in  taskSettings.json , the key  defaultEvaluationSolutions . This key will get copied directly to  defaultParams.json , where it will indicate the skeleton of what needs to be executed by the taskgrader. Some more information can be found in the  defaultParams.json  reference . Here this key contains:  \"defaultEvaluationSolutions\": [{\n    \"id\": \"@solutionId\",\n    \"compilationDescr\": {\n        \"language\": \"python\",\n        \"files\": [{\"name\": \"runner.py\",\n                   \"path\": \"$TASK_PATH/tests/gen/runner.py\"}],\n        \"dependencies\": [{\"name\": \"solution.py\",\n                          \"path\": \"@solutionPath\",\n                          \"content\": \"@solutionContent\"}]\n        },\n    \"compilationExecution\": \"@defaultSolutionCompParams\"}]  This example means to use  runner.py  as script to execute (hence in \"files\"), and to give the solution as dependency, stored in  solution.py  (whichever the original solution filename is, it will be renamed to  solution.py ).  In this key, we indicate what needs to be executed, and the values  '@solutionFilename', '@solutionLanguage', '@solutionDependencies', '@solutionPath', '@solutionContent'  will be replaced with the solution's name, language, dependencies, and its path or content. In this example task,  runner.py  is defined as the main program to execute, and our solution is passed as dependency of this program, and automatically named  solution.py  to be then imported by  runner.py .  You can test the task by running, from the task folder:   taskstarter.py test  for the normal test  taskstarter.py testsol tests/gen/sol-ok-py.py  for a good solution  taskstarter.py testsol tests/gen/sol-bad-py.py  for a bad solution   The testing tool  stdGrade  will automatically understand how to evaluate these solutions.  This example can be found in the  examples/taskRunner  folder.", 
            "title": "Runner example: solution skeleton (taskRunner)"
        }, 
        {
            "location": "/examples/#another-runner-example-counting-the-number-of-mathematical-operations", 
            "text": "In this example, we count the number of mathematical operations to make sure the solution doesn't do too many. To do this, we use a runner which modify the execution environment of the solution : it will replace the  int  function by one returning a  Number  class. This class is a custom class allowing us to count the number of operations and block some operations from being used (in this example, only addition and substraction are allowed, multiplication and division aren't).  The runner outputs the statistics at the end of the script, which are then read back by the checker.  This example can be found in the  examples/taskMaths  folder.", 
            "title": "Another runner example: counting the number of mathematical operations"
        }, 
        {
            "location": "/examples/#solution-checker-example-reading-the-solution-source-tasksolchecker", 
            "text": "In this task, the checker grades the solution source code.  For this example, the checker counts the number of characters of the solution (excluding comment lines and whitespaces), and gives a lower grade if the solution source is too long. (Of course it gives a grade of 0 if the solution doesn't output the correct answer.)  In order to have the checker be able to read the solution source, we use the  addFiles  key of its  runExecution  params; this key allows to add any file in the working folder during an execution. Here, it adds to the checker execution an access to the solution source, stored in file  solution . The checker can then read this file, along with the test case files, to accordingly grade the solution.  Here, we modify the key  defaultChecker  in order to add the file, like this:  \"defaultChecker\": {\n    \"compilationDescr\": {\n        \"language\": \"python2\",\n        \"files\": [{\n            \"name\": \"checker.py\",\n            \"path\": \"$TASK_PATH/tests/gen/checker.py\"\n            }],\n        \"dependencies\": []},\n    \"compilationExecution\": \"@defaultToolCompParams\",\n    \"runExecution\": {\n        \"memoryLimitKb\": 131072,\n        \"timeLimitMs\": 60000,\n        \"useCache\": true,\n        \"stdoutTruncateKb\": -1,\n        \"stderrTruncateKb\": -1,\n        \"addFiles\": [{\n            \"name\": \"solution\",\n            \"path\": \"@solutionPath\",\n            \"content\": \"@solutionContent\"\n            }],\n        \"getFiles\": []}}\n}  You can test the task by running, from the task folder:   taskstarter.py test  for the normal test, which will check each solution has the expected grade  taskstarter.py testsol tests/gen/sol-ok-c.c  for a good and short solution getting the perfect grade of 100 (right answer, less than 60 characters)  taskstarter.py testsol tests/gen/sol-long-c.c  for a solution too long getting a grade of 93 (right answer, more than 60 characters)  taskstarter.py testsol tests/gen/sol-bad-c.c  for a bad solution getting a grade of 0 (wrong answer)   This example can be found in the  examples/taskSolchecker  folder.", 
            "title": "Solution checker example: reading the solution source (taskSolchecker)"
        }, 
        {
            "location": "/examples/#compilation-checker-example-reading-the-compiler-output-taskcchecker", 
            "text": "In this task, the checker reads the compiler output. It then, as an example, displays it on his grading output; a more detailed checker could though read the compiler output to check for specific errors.  This is done by using again the  addFiles  key as done in the previous example, and adding the compiler output in the files to give to the checker:  \"addFiles\": [\n  {\n    \"name\": \"compilationStdout\",\n    \"path\": \"$BUILD_PATH/solutions/solution/stdout\"\n  }, {\n    \"name\": \"compilationStderr\",\n    \"path\": \"$BUILD_PATH/solutions/solution/stderr\"\n  }]  This will give the checker access to the compilation output through the files  compilationStdout  and  compilationStderr .  This example can be found in the  examples/taskCChecker  folder.", 
            "title": "Compilation checker example: reading the compiler output (taskCChecker)"
        }, 
        {
            "location": "/examples/#specific-inputoutput-files-examples-taskinputs", 
            "text": "In this task, the solution has to read from a file named  input , and write its answer to the file  output .  This is done, like previous examples, by using the  addFiles  key, to add for each test a new file  input  containing the new input we want to give to the solution.  In the  taskSettings.json  file, we use the  defaultEvaluationExecutions  to specify each test: set the contents of the  input  file, and specify we want to capture the file  output  (optional, allows to get its contents in the output JSON but unneeded for the checker to have access to this file).  This example can be found in the  examples/taskInputs  folder.", 
            "title": "Specific input/output files examples (taskInputs)"
        }, 
        {
            "location": "/examples/#turtle-example-using-and-saving-graphics-taskturtle", 
            "text": "In this task, the solution is a  turtle  script; here, its task is to simply to go to a specified position (but more advanced criterias can be written).  This task is complex for two reasons:   the base turtle library needs a graphic window to work (unavailable on an evaluation server), and we still need to \"isolate\" the solution execution  we save the resulting image as a PNG file", 
            "title": "Turtle example: using and saving graphics (taskTurtle)"
        }, 
        {
            "location": "/examples/#requirements", 
            "text": "To work, this task needs:   Xvfb, \"X virtual framebuffer\", a X server without display (to make turtle show graphics and then save them)  PIL, \"Python Imaging Library\", a library to handle images  Tkinter, used by the turtle for its graphics  Ghostscript, for PIL to read the saved output from Tk   On Debian, they can be installed with the following command:  apt-get install xvfb python-pil python-tk ghostscript  The task also needs the  checker.sh  to be whitelisted for use outside of isolate; this is done by adding its path in  CFG_NOISOLATE  in taskgrader's  config.py  file, for instance:  CFG_NOISOLATE = [\"/path/to/taskgrader/examples/taskTurtle/tests/gen/checker.sh\"]  (It must be an absolute path, as returned by Python's  os.path.abspath .)", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/#how-it-works", 
            "text": "This task works by saving all turtle commands executed by the solution, and then replaying them to generate the resulting PNG, and also to evaluate the criteria of task completion.", 
            "title": "How it works"
        }, 
        {
            "location": "/examples/#runnerpy", 
            "text": "The execution of solutions is wrapped in a  runner.py  execution.  The runner of this task defines a class,  LoggedTurtle , which logs all the turtle commands executed while inhibiting their graphics. It also modifies the solution source code to change all instances of the normal  Turtle  class to this  LoggedTurtle  class; and then executes the solution while logging all commands. It finally prints the full JSON-encoded log, whose entries contain the following elements:   ID number for the turtle (to allow multiple turtles)  Component, either 'nav' for the navigator, 'turtle' for other functions  Name of turtle's function called  args and kwargs of the call   See  turtleToPng.py  or  checker.py  for examples on how to use this log.", 
            "title": "runner.py"
        }, 
        {
            "location": "/examples/#checkersh", 
            "text": "This script has two sub-components:   turtleToPng.py  is a script generating the base64-encoded PNG image resulting from the execution of the turtle  checker.py  is the actual checker, using the turtle log to evaluate the criteria and grade the solution   As said above, this script must be whitelisted for executiong outside of isolate by the taskgrader. It will allow the script to run Xvfb and  turtleToPng.py  outside of isolate, as X displays cannot run inside isolate.  It will then use the taskgrader tool  tools/isolate-run.py  to run the actual checker  checker.py  inside isolate.", 
            "title": "checker.sh"
        }, 
        {
            "location": "/examples/#usage", 
            "text": "As usual, you can test the task by running, from the task folder:   taskstarter.py test  for the normal test, which will check each solution has the expected grade  taskstarter.py testsol tests/gen/sol-ok-py.py  for a good solution  taskstarter.py testsol tests/gen/sol-bad-py.py  for a bad solution   If you grade a solution against the task, the output JSON will contain a file  turtle_png.b64 , which is the base64-encoded PNG image resulting from each execution of the solution against a test case. It is in the  files  key of each  checker  report (in  /executions[idx]/testReports[idx] ). The file  view.html  in the task folder contains an example JavaScript to display it in a browser.  This example can be found in the  examples/taskTurtle  folder.", 
            "title": "Usage"
        }, 
        {
            "location": "/inputjson/", 
            "text": "Input JSON\n\n\nThis page describes the \ninput JSON\n, which is the JSON data sent to the taskgrader (on its standard input) to specify the full evaluation process.\n\n\nThis page is currently under writing, but you can check the input JSON format in the file \nschema_input.json\n.\n\n\nSimplified version\n\n\nWriting in progress\n\n\nWith the genJson tool, you can generate for each task a \ndefaultParams.json\n file\n which will contain default values for evaluations against that task. Once a task has its defaultParams generated, you can evaluate a solution with a simple input JSON like this:\n\n\n{\n    \"rootPath\": \"/some/path\",\n    \"taskPath\": \"/path/to/the/task\",\n    \"extraParams\": {\n        \"solutionFilename\": \"solution.c\",\n        \"solutionPath\": \"/home/user/solution.c\",\n        \"solutionLanguage\": \"c\",\n        \"solutionDependencies\": \"@defaultDependencies-c\"\n}}", 
            "title": "Input JSON"
        }, 
        {
            "location": "/inputjson/#input-json", 
            "text": "This page describes the  input JSON , which is the JSON data sent to the taskgrader (on its standard input) to specify the full evaluation process.  This page is currently under writing, but you can check the input JSON format in the file  schema_input.json .", 
            "title": "Input JSON"
        }, 
        {
            "location": "/inputjson/#simplified-version", 
            "text": "Writing in progress  With the genJson tool, you can generate for each task a  defaultParams.json  file  which will contain default values for evaluations against that task. Once a task has its defaultParams generated, you can evaluate a solution with a simple input JSON like this:  {\n    \"rootPath\": \"/some/path\",\n    \"taskPath\": \"/path/to/the/task\",\n    \"extraParams\": {\n        \"solutionFilename\": \"solution.c\",\n        \"solutionPath\": \"/home/user/solution.c\",\n        \"solutionLanguage\": \"c\",\n        \"solutionDependencies\": \"@defaultDependencies-c\"\n}}", 
            "title": "Simplified version"
        }, 
        {
            "location": "/tasksettings/", 
            "text": "taskSettings.json\n\n\nThe file \ntaskSettings.json\n contains information about the task it is present in, such as its components or its evaluation parameters.\n\n\nIt is used mainly by the genJson tool to generate the \ndefaultParams.json\n file\n. It is a JSON object containing key-value pairs.\n\n\nExample \n(other examples can be found in the \nexamples\n folder)\n:\n\n\n{\n\"sanitizer\": \"tests/gen/sanitizer.sh\",\n\"correctSolutions\": [{\n    \"path\": \"$TASK_PATH/tests/gen/sol-ok-c.c\",\n    \"language\": \"c\",\n    \"grade\": 100\n    }]\n}\n\n\n\nThe following list is the list of keys it accepts.\n\n\ngenerator, sanitizer, checker\n\n\n\n\nName: \ngenerator\n, \nsanitizer\n or \nchecker\n\n\nType: \nstring\n\n\nExample: \n\"tests/gen/sanitizer.py\"\n\n\n\n\nThe keys \ngenerator\n, \nsanitizer\n, \nchecker\n specify the path to each of these three components of the task, relatively to the task path.\n\n\nLanguage\n\n\n\n\nName: \nsanitizerLang\n or \ncheckerLang\n\n\nType: \nstring\n\n\nExample: \n\"cpp\"\n\n\n\n\nThe language of the sanitizer and the checker is normally automatically detected from the file extension, but can be specified manually with these two keys.\n\n\nDependencies\n\n\n\n\nName: \ngeneratorDeps\n, \nsanitizerDeps\n or \ncheckerDeps\n\n\nType: \nobject\n\n\nExample: \n{\"name\": \"libcheck.py\", \"path\": \"$TASK_PATH/tests/gen/libcheck.py\"}\n\n\n\n\nThese keys specify the dependencies needed for each of these three components of the task. Each object is a \nfileDescr\n as defined in the taskgrader's \nschema_input.json\n.\n\n\nNote that the generator dependencies are normally automatically detected, if they are in the same folder.\n\n\nextraDir\n\n\n\n\nName: \nextraDir\n\n\nType: \nstring\n\n\nExample: \n\"tests/files/\"\n\n\n\n\nThis key specify the folder, relative to the task, to scan for \nextraFiles\n, that is to say, test cases given as-is, without generator.\n\n\nignoreTests\n\n\n\n\nName: \nignoreTests\n\n\nType: \nlist of strings\n\n\nExample: \n[\"sample*\"]\n\n\n\n\nThis key is a list of glob-style filenames to ignore while scanning for test cases. These test cases will be ignored by genJson, and thus not be used for evaluation.\n\n\ncorrectSolutions\n\n\n\n\nName: \nignoreTests\n\n\nType: \nlist of objects\n\n\nExample: \n[{\"path\": \"$TASK_PATH/tests/gen/sol-ok-c.c\", \"language\": \"c\", \"grade\": 100}]\n\n\n\n\nThis key defines the \"correct solutions\", which are solutions which will be automatically evaluated against the task by genJson, when generating the \ndefaultParams\n. They allow to test the task is behaving properly. It is a list of objects, each having up to 4 keys:\n\n\n\n\n\n\n\n\nKey name\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npath\n\n\nrequired \nstring\n\n\nPath to the solution\n\n\n\n\n\n\nlanguage\n\n\noptional \nstring\n\n\nLanguage of the solution\n\n\n\n\n\n\ngrade\n\n\noptional \nint\n\n\nAverage grade expected for this solution\n\n\n\n\n\n\nnbtests\n\n\noptional \nint\n\n\nNumber of test cases this solution is expected to be evaluated against\n\n\n\n\n\n\n\n\ndefault*\n\n\nAny key name starting with \"default\" will be copied as-is to the \ndefaultParams.json\n file. Please check \nits documentation\n for more information about these keys.\n\n\noverrideParams\n\n\n\n\nName: \noverrideParams\n\n\nType: \nobject\n\n\nExample: \n{\"defaultFilterTests\": [], \"defaultDependencies-python2\": \"@defaultDependencies-python\"}\n\n\n\n\nThis key defines parameters to override in the generated \ndefaultParams.json\n file\n. The keys of this object will be copied as-is to the \ndefaultParams\n, possibly overwriting automatically generated ones.", 
            "title": "taskSettings.json"
        }, 
        {
            "location": "/tasksettings/#tasksettingsjson", 
            "text": "The file  taskSettings.json  contains information about the task it is present in, such as its components or its evaluation parameters.  It is used mainly by the genJson tool to generate the  defaultParams.json  file . It is a JSON object containing key-value pairs.  Example  (other examples can be found in the  examples  folder) :  {\n\"sanitizer\": \"tests/gen/sanitizer.sh\",\n\"correctSolutions\": [{\n    \"path\": \"$TASK_PATH/tests/gen/sol-ok-c.c\",\n    \"language\": \"c\",\n    \"grade\": 100\n    }]\n}  The following list is the list of keys it accepts.", 
            "title": "taskSettings.json"
        }, 
        {
            "location": "/tasksettings/#generator-sanitizer-checker", 
            "text": "Name:  generator ,  sanitizer  or  checker  Type:  string  Example:  \"tests/gen/sanitizer.py\"   The keys  generator ,  sanitizer ,  checker  specify the path to each of these three components of the task, relatively to the task path.", 
            "title": "generator, sanitizer, checker"
        }, 
        {
            "location": "/tasksettings/#language", 
            "text": "Name:  sanitizerLang  or  checkerLang  Type:  string  Example:  \"cpp\"   The language of the sanitizer and the checker is normally automatically detected from the file extension, but can be specified manually with these two keys.", 
            "title": "Language"
        }, 
        {
            "location": "/tasksettings/#dependencies", 
            "text": "Name:  generatorDeps ,  sanitizerDeps  or  checkerDeps  Type:  object  Example:  {\"name\": \"libcheck.py\", \"path\": \"$TASK_PATH/tests/gen/libcheck.py\"}   These keys specify the dependencies needed for each of these three components of the task. Each object is a  fileDescr  as defined in the taskgrader's  schema_input.json .  Note that the generator dependencies are normally automatically detected, if they are in the same folder.", 
            "title": "Dependencies"
        }, 
        {
            "location": "/tasksettings/#extradir", 
            "text": "Name:  extraDir  Type:  string  Example:  \"tests/files/\"   This key specify the folder, relative to the task, to scan for  extraFiles , that is to say, test cases given as-is, without generator.", 
            "title": "extraDir"
        }, 
        {
            "location": "/tasksettings/#ignoretests", 
            "text": "Name:  ignoreTests  Type:  list of strings  Example:  [\"sample*\"]   This key is a list of glob-style filenames to ignore while scanning for test cases. These test cases will be ignored by genJson, and thus not be used for evaluation.", 
            "title": "ignoreTests"
        }, 
        {
            "location": "/tasksettings/#correctsolutions", 
            "text": "Name:  ignoreTests  Type:  list of objects  Example:  [{\"path\": \"$TASK_PATH/tests/gen/sol-ok-c.c\", \"language\": \"c\", \"grade\": 100}]   This key defines the \"correct solutions\", which are solutions which will be automatically evaluated against the task by genJson, when generating the  defaultParams . They allow to test the task is behaving properly. It is a list of objects, each having up to 4 keys:     Key name  Type  Description      path  required  string  Path to the solution    language  optional  string  Language of the solution    grade  optional  int  Average grade expected for this solution    nbtests  optional  int  Number of test cases this solution is expected to be evaluated against", 
            "title": "correctSolutions"
        }, 
        {
            "location": "/tasksettings/#default", 
            "text": "Any key name starting with \"default\" will be copied as-is to the  defaultParams.json  file. Please check  its documentation  for more information about these keys.", 
            "title": "default*"
        }, 
        {
            "location": "/tasksettings/#overrideparams", 
            "text": "Name:  overrideParams  Type:  object  Example:  {\"defaultFilterTests\": [], \"defaultDependencies-python2\": \"@defaultDependencies-python\"}   This key defines parameters to override in the generated  defaultParams.json  file . The keys of this object will be copied as-is to the  defaultParams , possibly overwriting automatically generated ones.", 
            "title": "overrideParams"
        }, 
        {
            "location": "/defaultparams/", 
            "text": "defaultParams.json\n\n\nThe file \ndefaultParams.json\n contains \ngenerated\n information about the task it is present in, and is used to auto-fill some values for the taskgrader.\n\n\nIt simplifies input JSON writing by allowing to omit some values in it and letting the taskgrader grab them from the defaultParams. More information in the \ninput JSON documentation page\n.\n\n\nIt is generated by the genJson tool, and the stdGrade tool uses it for evaluations. It is a JSON object containing key-value pairs, and is not meant to be manually modified. Check the \ntaskSettings.json\n file\n to control the generation of this file.\n\n\nThe following list is the list of keys it contains. Note that the types are from the \nschema_input.json\n file, describing the input JSON schema.\n\n\nTask components information\n\n\nThese keys are the task components which were detected by the genJson tool.\n\n\n\n\n\n\n\n\nKey name\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndefaultToolCompParams\n\n\nexecutionParams\n\n\nCompilation parameters for the components of the task\n\n\n\n\n\n\ndefaultToolExecParams\n\n\nexecutionParams\n\n\nExecution parameters for the components of the task\n\n\n\n\n\n\ndefaultGenerator\n\n\nlist of objects\n\n\nGenerator for the task\n\n\n\n\n\n\ndefaultGeneration\n\n\nobject\n\n\nGeneration associated with the detected generator\n\n\n\n\n\n\ndefaultExtraTests\n\n\nlist of fileDescr\n\n\nTest cases given as-is\n\n\n\n\n\n\ndefaultFilterTests-[lang]\n\n\nlist of glob\n\n\nList of test cases for each language\n\n\n\n\n\n\ndefaultFilterTests\n\n\nlist of glob\n\n\nList of test cases for all languages (most \ndefaultFilterTests-[lang]\n are aliases to this one)\n\n\n\n\n\n\ndefaultDependencies-[lang]\n\n\nlist of fileDescr\n\n\nDetected dependencies for each language\n\n\n\n\n\n\ndefaultSanitizer\n\n\ncompileAndRunParams\n\n\nSanitizer for the task\n\n\n\n\n\n\ndefaultChecker\n\n\ncompileAndRunParams\n\n\nChecker for the task\n\n\n\n\n\n\n\n\nDefault evaluation values (defaultEvaluation* keys)\n\n\nThese keys correspond to (almost) each root key in the \ninput JSON\n for the taskgrader. They are used by the taskgrader if the corresponding key is omitted in the input JSON; check the \ndocumentation on input JSON\n for more information on how to use these keys. Here are the default values for all of them:\n\n\n\"defaultEvaluationGenerators\": [\"@defaultGenerator\"],\n\"defaultEvaluationGenerations\": [\"@defaultGeneration\"],\n\"defaultEvaluationExtraTests\": \"@defaultExtraTests\",\n\"defaultEvaluationSanitizer\": \"@defaultSanitizer\",\n\"defaultEvaluationChecker\": \"@defaultChecker\",\n\"defaultEvaluationSolutions\": [{\n    \"id\": \"@solutionId\",\n    \"compilationDescr\": {\n        \"language\": \"@solutionLanguage\",\n        \"files\": [{\"name\": \"@solutionFilename\",\n                   \"path\": \"@solutionPath\",\n                   \"content\": \"@solutionContent\"}],\n        \"dependencies\": \"@solutionDependencies\"\n        },\n    \"compilationExecution\": \"@defaultSolutionCompParams\"\n    }],\n\"defaultEvaluationExecutions\": [{\n    \"id\": \"@solutionExecId\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": \"@solutionFilterTests\",\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    }]\n\n\n\nTo change the task behavior, you generally need to tweak these \ndefaultEvaluation\n keys (through \ntaskSettings.json\n); for instance, the example in \nexamples/taskRunner\n changes the solution execution, check the \ndocumentation about this example\n for more information.\n\n\nSolution default values\n\n\nSome default values are stored to be used for the \ndefaultEvaluation\n keys if not specified in the input JSON.\n\n\n\n\n\n\n\n\nKey name\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndefaultSolutionCompParams\n\n\nexecutionParams\n\n\nCFG_TESTSOL_PARAMS\n\n\nCompilation parameters for the solution (defaults to those specified in \nCFG_TESTSOL_PARAMS\n in genJson's \nconfig.py\n)\n\n\n\n\n\n\ndefaultSolutionExecParams\n\n\nexecutionParams\n\n\nCFG_TESTSOL_PARAMS\n\n\nExecution parameters for the solution (same as above)\n\n\n\n\n\n\nsolutionDependencies\n\n\nlist of fileDescr\n\n\n[]\n\n\nList of dependencies for the solution (should be set to \n'@defaultDependencies-[lang]'\n in the input JSON)\n\n\n\n\n\n\nsolutionFilterTests\n\n\nlist of strings\n\n\n'@defaultFilterTests'\n\n\nList of test cases to use on the solution\n\n\n\n\n\n\nsolutionId\n\n\nstring\n\n\n\"solution\"\n\n\nInternal ID for the solution\n\n\n\n\n\n\nsolutionExecId\n\n\nstring\n\n\n\"execution\"\n\n\nInternal ID for the execution of the solution\n\n\n\n\n\n\n\n\nAlso, one of these two keys must be specified in the input JSON (if \nsolutionPath\n is filled, \nsolutionContent\n will be ignored):\n\n\n\n\n\n\n\n\nKey name\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsolutionPath\n\n\nstring\n\n\nPath to the solution\n\n\n\n\n\n\nsolutionContent\n\n\nstring\n\n\nContent of the solution\n\n\n\n\n\n\n\n\nOther information\n\n\n\n\n\n\n\n\nKey name\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrootPath\n\n\nstring\n\n\nValue of the \nrootPath\n which was used when the defaultParams were generated.\n\n\n\n\n\n\ngenJsonVersion\n\n\nstring\n\n\nVersion of genJson which generated these defaultParams.", 
            "title": "defaultParams.json"
        }, 
        {
            "location": "/defaultparams/#defaultparamsjson", 
            "text": "The file  defaultParams.json  contains  generated  information about the task it is present in, and is used to auto-fill some values for the taskgrader.  It simplifies input JSON writing by allowing to omit some values in it and letting the taskgrader grab them from the defaultParams. More information in the  input JSON documentation page .  It is generated by the genJson tool, and the stdGrade tool uses it for evaluations. It is a JSON object containing key-value pairs, and is not meant to be manually modified. Check the  taskSettings.json  file  to control the generation of this file.  The following list is the list of keys it contains. Note that the types are from the  schema_input.json  file, describing the input JSON schema.", 
            "title": "defaultParams.json"
        }, 
        {
            "location": "/defaultparams/#task-components-information", 
            "text": "These keys are the task components which were detected by the genJson tool.     Key name  Type  Description      defaultToolCompParams  executionParams  Compilation parameters for the components of the task    defaultToolExecParams  executionParams  Execution parameters for the components of the task    defaultGenerator  list of objects  Generator for the task    defaultGeneration  object  Generation associated with the detected generator    defaultExtraTests  list of fileDescr  Test cases given as-is    defaultFilterTests-[lang]  list of glob  List of test cases for each language    defaultFilterTests  list of glob  List of test cases for all languages (most  defaultFilterTests-[lang]  are aliases to this one)    defaultDependencies-[lang]  list of fileDescr  Detected dependencies for each language    defaultSanitizer  compileAndRunParams  Sanitizer for the task    defaultChecker  compileAndRunParams  Checker for the task", 
            "title": "Task components information"
        }, 
        {
            "location": "/defaultparams/#default-evaluation-values-defaultevaluation42-keys", 
            "text": "These keys correspond to (almost) each root key in the  input JSON  for the taskgrader. They are used by the taskgrader if the corresponding key is omitted in the input JSON; check the  documentation on input JSON  for more information on how to use these keys. Here are the default values for all of them:  \"defaultEvaluationGenerators\": [\"@defaultGenerator\"],\n\"defaultEvaluationGenerations\": [\"@defaultGeneration\"],\n\"defaultEvaluationExtraTests\": \"@defaultExtraTests\",\n\"defaultEvaluationSanitizer\": \"@defaultSanitizer\",\n\"defaultEvaluationChecker\": \"@defaultChecker\",\n\"defaultEvaluationSolutions\": [{\n    \"id\": \"@solutionId\",\n    \"compilationDescr\": {\n        \"language\": \"@solutionLanguage\",\n        \"files\": [{\"name\": \"@solutionFilename\",\n                   \"path\": \"@solutionPath\",\n                   \"content\": \"@solutionContent\"}],\n        \"dependencies\": \"@solutionDependencies\"\n        },\n    \"compilationExecution\": \"@defaultSolutionCompParams\"\n    }],\n\"defaultEvaluationExecutions\": [{\n    \"id\": \"@solutionExecId\",\n    \"idSolution\": \"@solutionId\",\n    \"filterTests\": \"@solutionFilterTests\",\n    \"runExecution\": \"@defaultSolutionExecParams\"\n    }]  To change the task behavior, you generally need to tweak these  defaultEvaluation  keys (through  taskSettings.json ); for instance, the example in  examples/taskRunner  changes the solution execution, check the  documentation about this example  for more information.", 
            "title": "Default evaluation values (defaultEvaluation* keys)"
        }, 
        {
            "location": "/defaultparams/#solution-default-values", 
            "text": "Some default values are stored to be used for the  defaultEvaluation  keys if not specified in the input JSON.     Key name  Type  Default  Description      defaultSolutionCompParams  executionParams  CFG_TESTSOL_PARAMS  Compilation parameters for the solution (defaults to those specified in  CFG_TESTSOL_PARAMS  in genJson's  config.py )    defaultSolutionExecParams  executionParams  CFG_TESTSOL_PARAMS  Execution parameters for the solution (same as above)    solutionDependencies  list of fileDescr  []  List of dependencies for the solution (should be set to  '@defaultDependencies-[lang]'  in the input JSON)    solutionFilterTests  list of strings  '@defaultFilterTests'  List of test cases to use on the solution    solutionId  string  \"solution\"  Internal ID for the solution    solutionExecId  string  \"execution\"  Internal ID for the execution of the solution     Also, one of these two keys must be specified in the input JSON (if  solutionPath  is filled,  solutionContent  will be ignored):     Key name  Type  Description      solutionPath  string  Path to the solution    solutionContent  string  Content of the solution", 
            "title": "Solution default values"
        }, 
        {
            "location": "/defaultparams/#other-information", 
            "text": "Key name  Type  Description      rootPath  string  Value of the  rootPath  which was used when the defaultParams were generated.    genJsonVersion  string  Version of genJson which generated these defaultParams.", 
            "title": "Other information"
        }, 
        {
            "location": "/errors/", 
            "text": "Error messages\n\n\nIsolate is not properly installed\n\n\nThis error message happens when \nisolate\n, the tool used to isolate solution executions and gather metrics, was not properly installed. The taskgrader will fall back to a normal execution, which means the execution will not be isolated (allowing the solution to access the whole filesystem, or communicate over the network, for instance), and the taskgrader will not be able to tell how much time and memory the execution used. It's okay for a test environment, but \nisolate\n needs to be configured properly for a contest environment.\n\n\nThe script \ninstall.sh\n normally takes care of installing \nisolate\n properly; if not, try launching it again and looking for any error message related to \nisolate\n.\n\n\nUnable to import jsonschema\n\n\nThe taskgrader uses \njsonschema\n for input and output JSON validation. It should normally be downloaded by the \ninstall.sh\n script, but it may fail if \ngit\n is not installed. This validation is not mandatory, but if the input JSON is not valid, the taskgrader will most likely crash. The validation helps knowing which JSONs are invalid and why.\n\n\nIf \npip\n is available, you can install jsonschema automatically with \npip install jsonschema\n, alternatively you can download it manually from the \njsonschema GitHub repository\n.", 
            "title": "Error messages"
        }, 
        {
            "location": "/errors/#error-messages", 
            "text": "", 
            "title": "Error messages"
        }, 
        {
            "location": "/errors/#isolate-is-not-properly-installed", 
            "text": "This error message happens when  isolate , the tool used to isolate solution executions and gather metrics, was not properly installed. The taskgrader will fall back to a normal execution, which means the execution will not be isolated (allowing the solution to access the whole filesystem, or communicate over the network, for instance), and the taskgrader will not be able to tell how much time and memory the execution used. It's okay for a test environment, but  isolate  needs to be configured properly for a contest environment.  The script  install.sh  normally takes care of installing  isolate  properly; if not, try launching it again and looking for any error message related to  isolate .", 
            "title": "Isolate is not properly installed"
        }, 
        {
            "location": "/errors/#unable-to-import-jsonschema", 
            "text": "The taskgrader uses  jsonschema  for input and output JSON validation. It should normally be downloaded by the  install.sh  script, but it may fail if  git  is not installed. This validation is not mandatory, but if the input JSON is not valid, the taskgrader will most likely crash. The validation helps knowing which JSONs are invalid and why.  If  pip  is available, you can install jsonschema automatically with  pip install jsonschema , alternatively you can download it manually from the  jsonschema GitHub repository .", 
            "title": "Unable to import jsonschema"
        }, 
        {
            "location": "/moreinfo/", 
            "text": "Further information\n\n\nHow does it work?\n\n\nHere's a description of the evaluation process that taskgrader does, managed by the function \nevaluation(evaluationParams)\n.\n\n\n\n\nevaluationParams\n is the input JSON\n\n\nA build folder is created for the evaluation\n\n\nThe \ndefaultParams.json\n file from the task is read and its variables added\n\n\nA \ndictWithVars\n is created to add the variables, and returns the JSON data as if variables were replaced by their values\n\n\ngenerators\n are compiled\n\n\ngenerations\n describe how \ngenerators\n are to be executed in order to generate all the test files and optional libraries\n\n\nextraTests\n are added into the tests pool\n\n\nThe \nsanitizer\n and the \nchecker\n are compiled\n\n\nThe \nsolutions\n are compiled\n\n\nAll \nexecutions\n are done for the solutions\n\n\nThe full evaluation report is returned on standard output\n\n\n\n\nExecutions\n\n\nEach execution is the grading of one solution against multiple test files. For each \nexecution\n:\n\n Test files corresponding to \nfilterTests\n are selected, then for each test file:\n\n It passes first the \nsanitizer\n test\n\n Then the solution is executed, with the test file as standard input and the output saved\n\n Finally the \nchecker\n grades the solution according to its output on that particular test file\n\n\nfilterTests\n is a list of globs (as \n\"test*.in\"\n or \n\"mytest.in\"\n) selecting test files to use among all the test files generated by the generators, and the \nextraTests\n given. One can specify directly test files into this array to use only specific ones.\n\n\nEvaluation components\n\n\nThe evaluation is made against a task which has multiple components.\n\n\nGenerators\n\n\nThe \ngenerators\n are generating the testing environment. They are executed, optionally with various parameters, to generate files, which can be:\n\n\n\n\ntest files: inputs for the solution, and if necessary, expected output results\n\n\nlibraries, for the compilation and execution of solutions\n\n\n\n\nSome of these files can be passed directly in the evaluation JSON, without the need of a generator.\n\n\nSanitizer\n\n\nThe \nsanitizer\n checks whether a test input is valid. It expects the test input on its stdin, and its exit code indicates the validity of the data.\n\n\nChecker\n\n\nThe \nchecker\n checks whether the output of a solution corresponds to the expected result. It expects three arguments on the command line:\n\n\n\n\ntest.solout\n the solution output\n\n\ntest.in\n the reference input\n\n\ntest.out\n the reference output\n\n\n\n\nAll checkers are passed these three arguments, whether they use it or not. The checker outputs the grading of the solution; its exit code can indicate an error while checking (invalid arguments, missing files, ...).\n\n\nTools\n\n\nVarious tools are available in the subfolder \ntools\n. They can be configured with their respective \nconfig.py\n files.\n\n\nCreating a task\n\n\ntaskstarter.py\n helps task writers create and modify simple tasks. This simple tool is meant as a starting point for people not knowing how the taskgrader works but willing to write a task, and helps them through documented steps. It allows to do some operations in tasks folders, such as creating the base skeleton, giving some help on various components and testing the task. This tool creates a \ntaskSettings.json\n in the task folder, that \ngenJson.py\n can then use to create a \ndefaultParams.json\n accordingly. Read the \"Getting started on writing a task\" section for more information.\n\n\nPreparing a task for grading\n\n\ngenJson.py\n analyses tasks and creates the \ndefaultParams.json\n file for them. It will read the \ntaskSettings.json\n file in each task for some settings and try to automatically detect other settings.\n\n\ntaskSettings.json\n\n\nThe \ntaskSettings.json\n is JSON data giving some parameters about the task, for use by \ngenJson.py\n. It has the following keys:\n\n\n\n\ngenerator\n: path to the generator of the task\n\n\ngeneratorDeps\n: dependencies for the generator (list of fileDescr, see the input JSON schema for more information)\n\n\nsanitizer\n, \nsanitizerDeps\n, \nsanitizerLang\n: sanitizer of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)\n\n\nchecker\n, \ncheckerDeps\n, \ncheckerLang\n: checker of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)\n\n\nextraDir\n: folder with extra files (input test files and/or libraries)\n\n\noverrideParams\n: JSON data to be copied directly into \ndefaultParams.json\n, will replace any key with the same name from \ngenJson.py\n generated JSON data\n\n\ncorrectSolutions\n: list of solutions known as working with the task, will be tested by \ngenJson.py\n which will check whether they get the right results. Each solution must have the following keys: \npath\n, \nlang\n and \ngrade\n (the numerical grade the solution is supposed to get).\n\n\n\n\ndefaultParams.json\n\n\nThe \ndefaultParams.json\n is a task file giving some information about the task, must be JSON data pairing the following keys with the right objects:\n\n\n\n\nrootPath\n: the root path of the files\n\n\ndefaultGenerator\n: a default generator\n\n\ndefaultGeneration\n: the default generation for the default generator\n\n\nextraTests\n (optional): some extra tests\n\n\ndefaultSanitizer\n: the default sanitizer\n\n\ndefaultChecker\n: the default checker\n\n\ndefaultDependencies-[language]\n (optional): default dependencies for that language; if not defined, it will fallback to \ndefaultDependencies\n or to an empty list\n\n\ndefaultFilterTests-[language]\n (optional): default glob-style filters for the tests for that language; if not defined, it will fallback to \ndefaultFilterTests\n or to an empty list\n\n\n\n\nGrading a solution\n\n\nstdGrade.sh\n allows to easily grade a solution. The task path must be the current directory, or must be specified with \n-p\n. It will expect to have a \ndefaultParams.json\n file in the task directory, describing the task with some variables. Note that it's meant for fast and simple grading of solutions, it doesn't give a full control over the evaluation process. \nstdGrade.sh\n is a shortcut to two utilities present in its folder, for more options, see \ngenStdTaskJson.py -h\n.\n\n\nBasic usage: \nstdGrade.sh [SOLUTION]...\n from a task folder.\n\n\nExit codes\n\n\nThe taskgrader will return the following exit codes:\n\n\n\n\n0\n if the evaluation took place without error\n\n\n1\n if an error with the evaluation happened, usually because of the evaluation parameters themselves\n\n\n2\n if there was a temporary error, meaning the same evaluation should be tried again at a later time\n\n\n3\n if the evaluation needed a language which is not supported, or which lacks a dependency to compile\n\n\n\n\nInternals (for developers)\n\n\nevaluation\n is the evaluation process. It reads an input JSON and preprocesses it to replace the variables.\n\n\nEach program is defined as an instance of the class Program, that we \ncompile\n, then \nprepareExecution\n to set the execution parameters, then \nexecute\n with the proper parameters.\n\n\nLanguages are set as classes which define two functions: \ngetSource\n which defines how to search for some dependencies for this language, and \ncompile\n which is the compilation process.\n\n\nThe cache is handled by various Cache classes, each storing the cache parameters for a specific program and giving access to the various cache folders corresponding to compilation or execution of said programs.\n\n\nUpdate documentation\n\n\nThe documentation is in the \ndocs/\n folder. It is written in MarkDown, and formatted into HTML by \nMkDocs\n. To update the documentation:\n\n\n\n\npip install mkdocs\n to install mkdocs locally with pip\n\n\nmkdocs serve\n to preview (live) your changes\n\n\nmkdocs build\n to build a local HTML version of the documentation\n\n\nmkdocs gh-deploy\n to build the HTML version and upload it directly to the \ntaskgrader GitHub Pages", 
            "title": "Further information"
        }, 
        {
            "location": "/moreinfo/#further-information", 
            "text": "", 
            "title": "Further information"
        }, 
        {
            "location": "/moreinfo/#how-does-it-work", 
            "text": "Here's a description of the evaluation process that taskgrader does, managed by the function  evaluation(evaluationParams) .   evaluationParams  is the input JSON  A build folder is created for the evaluation  The  defaultParams.json  file from the task is read and its variables added  A  dictWithVars  is created to add the variables, and returns the JSON data as if variables were replaced by their values  generators  are compiled  generations  describe how  generators  are to be executed in order to generate all the test files and optional libraries  extraTests  are added into the tests pool  The  sanitizer  and the  checker  are compiled  The  solutions  are compiled  All  executions  are done for the solutions  The full evaluation report is returned on standard output", 
            "title": "How does it work?"
        }, 
        {
            "location": "/moreinfo/#executions", 
            "text": "Each execution is the grading of one solution against multiple test files. For each  execution :  Test files corresponding to  filterTests  are selected, then for each test file:  It passes first the  sanitizer  test  Then the solution is executed, with the test file as standard input and the output saved  Finally the  checker  grades the solution according to its output on that particular test file  filterTests  is a list of globs (as  \"test*.in\"  or  \"mytest.in\" ) selecting test files to use among all the test files generated by the generators, and the  extraTests  given. One can specify directly test files into this array to use only specific ones.", 
            "title": "Executions"
        }, 
        {
            "location": "/moreinfo/#evaluation-components", 
            "text": "The evaluation is made against a task which has multiple components.", 
            "title": "Evaluation components"
        }, 
        {
            "location": "/moreinfo/#generators", 
            "text": "The  generators  are generating the testing environment. They are executed, optionally with various parameters, to generate files, which can be:   test files: inputs for the solution, and if necessary, expected output results  libraries, for the compilation and execution of solutions   Some of these files can be passed directly in the evaluation JSON, without the need of a generator.", 
            "title": "Generators"
        }, 
        {
            "location": "/moreinfo/#sanitizer", 
            "text": "The  sanitizer  checks whether a test input is valid. It expects the test input on its stdin, and its exit code indicates the validity of the data.", 
            "title": "Sanitizer"
        }, 
        {
            "location": "/moreinfo/#checker", 
            "text": "The  checker  checks whether the output of a solution corresponds to the expected result. It expects three arguments on the command line:   test.solout  the solution output  test.in  the reference input  test.out  the reference output   All checkers are passed these three arguments, whether they use it or not. The checker outputs the grading of the solution; its exit code can indicate an error while checking (invalid arguments, missing files, ...).", 
            "title": "Checker"
        }, 
        {
            "location": "/moreinfo/#tools", 
            "text": "Various tools are available in the subfolder  tools . They can be configured with their respective  config.py  files.", 
            "title": "Tools"
        }, 
        {
            "location": "/moreinfo/#creating-a-task", 
            "text": "taskstarter.py  helps task writers create and modify simple tasks. This simple tool is meant as a starting point for people not knowing how the taskgrader works but willing to write a task, and helps them through documented steps. It allows to do some operations in tasks folders, such as creating the base skeleton, giving some help on various components and testing the task. This tool creates a  taskSettings.json  in the task folder, that  genJson.py  can then use to create a  defaultParams.json  accordingly. Read the \"Getting started on writing a task\" section for more information.", 
            "title": "Creating a task"
        }, 
        {
            "location": "/moreinfo/#preparing-a-task-for-grading", 
            "text": "genJson.py  analyses tasks and creates the  defaultParams.json  file for them. It will read the  taskSettings.json  file in each task for some settings and try to automatically detect other settings.", 
            "title": "Preparing a task for grading"
        }, 
        {
            "location": "/moreinfo/#tasksettingsjson", 
            "text": "The  taskSettings.json  is JSON data giving some parameters about the task, for use by  genJson.py . It has the following keys:   generator : path to the generator of the task  generatorDeps : dependencies for the generator (list of fileDescr, see the input JSON schema for more information)  sanitizer ,  sanitizerDeps ,  sanitizerLang : sanitizer of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)  checker ,  checkerDeps ,  checkerLang : checker of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)  extraDir : folder with extra files (input test files and/or libraries)  overrideParams : JSON data to be copied directly into  defaultParams.json , will replace any key with the same name from  genJson.py  generated JSON data  correctSolutions : list of solutions known as working with the task, will be tested by  genJson.py  which will check whether they get the right results. Each solution must have the following keys:  path ,  lang  and  grade  (the numerical grade the solution is supposed to get).", 
            "title": "taskSettings.json"
        }, 
        {
            "location": "/moreinfo/#defaultparamsjson", 
            "text": "The  defaultParams.json  is a task file giving some information about the task, must be JSON data pairing the following keys with the right objects:   rootPath : the root path of the files  defaultGenerator : a default generator  defaultGeneration : the default generation for the default generator  extraTests  (optional): some extra tests  defaultSanitizer : the default sanitizer  defaultChecker : the default checker  defaultDependencies-[language]  (optional): default dependencies for that language; if not defined, it will fallback to  defaultDependencies  or to an empty list  defaultFilterTests-[language]  (optional): default glob-style filters for the tests for that language; if not defined, it will fallback to  defaultFilterTests  or to an empty list", 
            "title": "defaultParams.json"
        }, 
        {
            "location": "/moreinfo/#grading-a-solution", 
            "text": "stdGrade.sh  allows to easily grade a solution. The task path must be the current directory, or must be specified with  -p . It will expect to have a  defaultParams.json  file in the task directory, describing the task with some variables. Note that it's meant for fast and simple grading of solutions, it doesn't give a full control over the evaluation process.  stdGrade.sh  is a shortcut to two utilities present in its folder, for more options, see  genStdTaskJson.py -h .  Basic usage:  stdGrade.sh [SOLUTION]...  from a task folder.", 
            "title": "Grading a solution"
        }, 
        {
            "location": "/moreinfo/#exit-codes", 
            "text": "The taskgrader will return the following exit codes:   0  if the evaluation took place without error  1  if an error with the evaluation happened, usually because of the evaluation parameters themselves  2  if there was a temporary error, meaning the same evaluation should be tried again at a later time  3  if the evaluation needed a language which is not supported, or which lacks a dependency to compile", 
            "title": "Exit codes"
        }, 
        {
            "location": "/moreinfo/#internals-for-developers", 
            "text": "evaluation  is the evaluation process. It reads an input JSON and preprocesses it to replace the variables.  Each program is defined as an instance of the class Program, that we  compile , then  prepareExecution  to set the execution parameters, then  execute  with the proper parameters.  Languages are set as classes which define two functions:  getSource  which defines how to search for some dependencies for this language, and  compile  which is the compilation process.  The cache is handled by various Cache classes, each storing the cache parameters for a specific program and giving access to the various cache folders corresponding to compilation or execution of said programs.", 
            "title": "Internals (for developers)"
        }, 
        {
            "location": "/moreinfo/#update-documentation", 
            "text": "The documentation is in the  docs/  folder. It is written in MarkDown, and formatted into HTML by  MkDocs . To update the documentation:   pip install mkdocs  to install mkdocs locally with pip  mkdocs serve  to preview (live) your changes  mkdocs build  to build a local HTML version of the documentation  mkdocs gh-deploy  to build the HTML version and upload it directly to the  taskgrader GitHub Pages", 
            "title": "Update documentation"
        }
    ]
}