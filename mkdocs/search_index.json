{
    "docs": [
        {
            "location": "/", 
            "text": "Taskgrader\n\n\nThe taskgrader tool manages every step of grading a contest task, from the generation of test data to the grading of a solution output.\n\n\nIt allows for a wide variety of contest task types and languages to be evaluated, and is meant to be used both locally for tests and in contest evaluation settings.\n\n\nIt uses \nisolate\n as a sandbox to run solutions to limit execution time and memory, as well as their access to the environment.\n\n\nThis documentation covers:\n\n\n\n\nInstallation\n\n\nGetting started\n and basic usage\n\n\nHow to write tasks\n for use with the taskgrader\n\n\nSome error messages\n and their meaning\n\n\nFurther information\n on the taskgrader internals", 
            "title": "Home"
        }, 
        {
            "location": "/#taskgrader", 
            "text": "The taskgrader tool manages every step of grading a contest task, from the generation of test data to the grading of a solution output.  It allows for a wide variety of contest task types and languages to be evaluated, and is meant to be used both locally for tests and in contest evaluation settings.  It uses  isolate  as a sandbox to run solutions to limit execution time and memory, as well as their access to the environment.  This documentation covers:   Installation  Getting started  and basic usage  How to write tasks  for use with the taskgrader  Some error messages  and their meaning  Further information  on the taskgrader internals", 
            "title": "Taskgrader"
        }, 
        {
            "location": "/install/", 
            "text": "Dependencies\n\n\nOn many distributions, the required dependencies are already installed.\n\n\nOn \nDebian or Ubuntu\n, the recommended dependencies are:\n\n\napt-get install build-essential git python3 sudo\n\n\n\nSome additional dependencies are required to support all features and languages, on Debian or Ubuntu:\n\n\napt-get install fp-compiler gcj-4.9 nodejs php5-cli\n\n\n\nOn \nFedora\n, the recommended dependencies are:\n\n\ndnf install @development-tools glibc-static libstdc++-static\n\n\n\nSome systems don't provide the \ngcj\n shortcut, in that case make a symlink to your version of \ngcj\n, such as:\n\n\nln -s /usr/bin/gcj-4.9 /usr/bin/gcj\n\n\n\nControl groups (for contest environments)\n\n\nIn a contest environment, you may want control groups enabled in your kernel:\n\n\napt-get install cgroup-tools\n\n\n\nOn some kernels, you might need to (re)activate the memory subsystem of control groups (on Debian, you can check whether the folder \n/sys/fs/cgroup/memory\n is present).\n\n\nYou can do this by using the \ncgroup_enable=memory\n kernel option. On many systems, you can do that by editing \n/etc/default/grub\n to add:\n\n\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory\"\n\n\n\nand then executing \nupdate-grub\n as root. Once enabled, set \nCFG_CONTROLGROUPS\n to \nTrue\n in \nconfig.py\n (after installation) to enable their usage within the taskgrader.\n\n\nSome more information can be found in the \nisolate man page\n.\n\n\nInstallation\n\n\nExecute \ninstall.sh\n in the taskgrader directory to install, as the user who will be running the taskgrader. It will help you install everything.\n\n\nThe installation of 'isolate' needs root access, and ability to have files owned by root with setuid on the current directory (doesn't work with remote folders such as NFS). If you cannot, you won't be able to use 'isolate', but the taskgrader will still work.\n\n\nIf needed, edit \nconfig.py\n to suit your needs; however default values will work for simple tests.\n\n\nTesting\n\n\nAfter configuration, you can test that the taskgrader is configured properly and is behaving as expected by running \ntests/test.py\n. By default, it will run all tests and give you a summary. Full usage instructions are given by \ntest.py -h\n.\n\n\nIf you didn't install dependencies for all languages, some tests will fail.", 
            "title": "Installing"
        }, 
        {
            "location": "/install/#dependencies", 
            "text": "On many distributions, the required dependencies are already installed.  On  Debian or Ubuntu , the recommended dependencies are:  apt-get install build-essential git python3 sudo  Some additional dependencies are required to support all features and languages, on Debian or Ubuntu:  apt-get install fp-compiler gcj-4.9 nodejs php5-cli  On  Fedora , the recommended dependencies are:  dnf install @development-tools glibc-static libstdc++-static  Some systems don't provide the  gcj  shortcut, in that case make a symlink to your version of  gcj , such as:  ln -s /usr/bin/gcj-4.9 /usr/bin/gcj", 
            "title": "Dependencies"
        }, 
        {
            "location": "/install/#control-groups-for-contest-environments", 
            "text": "In a contest environment, you may want control groups enabled in your kernel:  apt-get install cgroup-tools  On some kernels, you might need to (re)activate the memory subsystem of control groups (on Debian, you can check whether the folder  /sys/fs/cgroup/memory  is present).  You can do this by using the  cgroup_enable=memory  kernel option. On many systems, you can do that by editing  /etc/default/grub  to add:  GRUB_CMDLINE_LINUX=\"cgroup_enable=memory\"  and then executing  update-grub  as root. Once enabled, set  CFG_CONTROLGROUPS  to  True  in  config.py  (after installation) to enable their usage within the taskgrader.  Some more information can be found in the  isolate man page .", 
            "title": "Control groups (for contest environments)"
        }, 
        {
            "location": "/install/#installation", 
            "text": "Execute  install.sh  in the taskgrader directory to install, as the user who will be running the taskgrader. It will help you install everything.  The installation of 'isolate' needs root access, and ability to have files owned by root with setuid on the current directory (doesn't work with remote folders such as NFS). If you cannot, you won't be able to use 'isolate', but the taskgrader will still work.  If needed, edit  config.py  to suit your needs; however default values will work for simple tests.", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#testing", 
            "text": "After configuration, you can test that the taskgrader is configured properly and is behaving as expected by running  tests/test.py . By default, it will run all tests and give you a summary. Full usage instructions are given by  test.py -h .  If you didn't install dependencies for all languages, some tests will fail.", 
            "title": "Testing"
        }, 
        {
            "location": "/start/", 
            "text": "Executing\n\n\nThe taskgrader itself can be executed with\n\n\npython taskgrader.py\n\n\n\nIt will wait for an input JSON on its standard input, then proceed to evaluation and then output the result JSON on standard output. Read \nschema_input.json\n and \nschema_output.json\n for a description of the expected formats. Various tools described later can help you write this input JSON.\n\n\nVerbosity options are available, use \ntaskgrader.py -h\n for more help.\n\n\nExample usage\n\n\nSome commands you can try:\n\n\n./taskgrader.py \n examples/testinput.json\n./taskgrader.py \n examples/testinput.json | tools/stdGrade/summarizeResults.py\n\n\n\nFirst command will execute the taskgrader on an example evaluation described by \nexamples/testinput.json\n; it will output the result JSON, which isn't very human-readable. The second command will pass that output JSON to \nsummarizeResults.py\n, a simple tool to show the results in a more human-readable way.\n\n\ntools/taskstarter/taskstarter.py init mynewtask\n\n\n\nThis command will start a new task in the folder \nmynewtask\n; use it if you want to write a task for use with the taskgrader. It will ask you a few questions to guide you through the various components of a task, and write a base task with some example files. The next section describes that in more detail.\n\n\ncd examples/task3 ; ../../tools/taskstarter/taskstarter.py test\n\n\n\nA task may be tested as shown with this command. Here it will test the example \ntask3\n.\n\n\nMore details on usage can be found through this documentation.", 
            "title": "Getting started"
        }, 
        {
            "location": "/start/#executing", 
            "text": "The taskgrader itself can be executed with  python taskgrader.py  It will wait for an input JSON on its standard input, then proceed to evaluation and then output the result JSON on standard output. Read  schema_input.json  and  schema_output.json  for a description of the expected formats. Various tools described later can help you write this input JSON.  Verbosity options are available, use  taskgrader.py -h  for more help.", 
            "title": "Executing"
        }, 
        {
            "location": "/start/#example-usage", 
            "text": "Some commands you can try:  ./taskgrader.py   examples/testinput.json\n./taskgrader.py   examples/testinput.json | tools/stdGrade/summarizeResults.py  First command will execute the taskgrader on an example evaluation described by  examples/testinput.json ; it will output the result JSON, which isn't very human-readable. The second command will pass that output JSON to  summarizeResults.py , a simple tool to show the results in a more human-readable way.  tools/taskstarter/taskstarter.py init mynewtask  This command will start a new task in the folder  mynewtask ; use it if you want to write a task for use with the taskgrader. It will ask you a few questions to guide you through the various components of a task, and write a base task with some example files. The next section describes that in more detail.  cd examples/task3 ; ../../tools/taskstarter/taskstarter.py test  A task may be tested as shown with this command. Here it will test the example  task3 .  More details on usage can be found through this documentation.", 
            "title": "Example usage"
        }, 
        {
            "location": "/writing/", 
            "text": "Getting started on writing a task\n\n\nUse \ntaskstarter.py init [taskpath]\n to interactively create a new task.\n\n\nA \"task\" is a set of programs and files representing the problem the solutions will be evaluated against:\n\n\n\n\nthe test cases (input files and the associated expected output),\n\n\nthe libraries the solutions can use\n\n\nan optional generator which generates these two types of files\n\n\na sanitizer, checking the input files are in the required format\n\n\nthe checker, grading each solution's output\n\n\n\n\nThe script \ntools/taskstarter/taskstarter.py\n can assist with writing a task; use \ntaskstarter.py help\n to see the available commands.\n\n\nHere are some examples based around a simple problem: the program is given a number as input, and must output the double of the number. These example tasks can be found in the \nexamples\n folder.\n\n\nExample 1: only test cases\n\n\nA task can be just test cases. The task can be built and tested like this:\n\n\n\n\nWe start our task in a folder with \ntaskstarter.py init\n (answering 'no' to all questions); it will give us a base task structure we can use as reference\n\n\nWe put the test cases input files \ntest1.in\n and \ntest2.in\n in the subfolder \ntests/files/\n\n\nWe put in the same subfolder \ntest1.out\n and \ntest2.out\n, the correct output of these test cases\n\n\nWe can write a valid solution, for instance \nsol-ok-c.c\n in this example\n\n\nWe can test this solution with \ntaskstarter.py testsol tests/gen/sol-ok-c.c\n, it will say the solution got a grade of 100 for each test\n\n\n\n\nWhen the task is only test cases, the tools will use default programs as sanitizer and checker. The solution will get a perfect grade of 100 if its output is the expected output (\ntest1.out\n for the test case \ntest1.in\n, ...), or the minimum grade of 0 if its output is different.\n\n\nThis example can be found in the \nexamples/task1\n folder.\n\n\nAdding correct solutions to our task\n\n\nOnce our task is almost ready, we can add \"correct solutions\": they are known solutions which have known results, such as always giving a good answer or always giving a bad answer.\n\n\nIt allows to test against regressions, that is to say that after modifications, that the task still gives the right grade to our known solutions.\n\n\nUsing again our example task 1, we can add correct solutions like this:\n\n\ntaskstarter.py addsol -g 100 -l c tests/gen/sol-ok-c.c\n\n\n\nIt means that each time we'll test the task, the solution \nsol-ok-c.c\n will be automatically evaluated against the task, and \n-g 100\n means we expect the solution to get a perfect grade of 100 each time.\n\n\nWe can also add an invalid solution, that is to say, a solution who will get the minimum grade of 0 each time. In this example, we have such a solution, \nsol-bad-c.c\n. We add it as a \"bad solution\" like this:\n\n\ntaskstarter.py addsol -g 0 -l c tests/gen/sol-bad-c.c\n\n\n\nwhich means that we expect it to get a grade of 0 on each test.\n\n\nWe can finally test the task by executing, in the task folder,\n\n\ntaskstarter.py test\n\n\n\nin the task folder. It will output:\n\n\nTest successful on 2 correctSolutions with up to 2 test cases.\n\n\n\nwhich means the task successfully compiled, that the evaluation of the good solution \nsol-ok-c.c\n got a perfect grade of 100 each time, and that the evaluation of the bad solution \nsol-bad-c.c\n got a bad grade of 0 each time.\n\n\nExample 2: adding a sanitizer and a checker\n\n\nIt's generally recommended to use a sanitizer and a checker: the sanitizer will ensure only valid input test files are given to solutions, especially in cases where contestants are allowed to use their own test files as examples; the checker can give a more precise grade to solutions, and also handle cases where the solution output is not in the exact format expected.\n\n\nWe add a sanitizer and a checker to our test like this:\n\n\n\n\nWe use the task made in the last example\n\n\nWe write a script \nsanitizer.sh\n which takes as input the test case, and sets its exit code to 0 if it's valid, 1 if it's not\n\n\nWe write a script \nchecker.sh\n which takes three arguments, \ntest.in\n the test case input, \ntest.solout\n the solution output, \ntest.out\n the reference (expected) output; and gives a grade to the solution based on its output\n\n\nWe add the sanitizer and the checker to the task with \ntaskstarter.py add checker tests/gen/checker.sh\n and \ntaskstarter.py add sanitizer tests/gen/sanitizer.sh\n\n\nFinally, we test the task with \ntaskstarter.py test\n. It will tell us whether our programs were detected, compiled and executed successfully, and whether the \"correct solution\" we defined passed the test and got the expected grade of 100.\n\n\n\n\nThe sanitizer and the checker can be written in any language supported by the taskgrader.\n\n\nThis example can be found in the \nexamples/task2\n folder.\n\n\nExample 3: adding libraries\n\n\nThis example is a task with libraries intended for usage by solutions. It's pretty simple to add libraries:\n\n\n\n\nWe write a library \nlib.h\n intended for usage with solution in the C language\n\n\nWe put it in \ntests/files/lib/c/lib.h\n\n\nFinally, we test the task with \ntaskstarter.py test\n.\n\n\n\n\nAs long as the libraries are in the right folder (by default, \ntests/files/lib/[language]/[library.ext]\n), they will be automatically detected by \ngenJson\n and added to the default dependencies for that language. All solutions of that language will have the library available for importation without any configuration necessry.\n\n\nThis example can be found in the \nexamples/task3\n folder.\n\n\nExample 4: adding a generator\n\n\nIt can be handy to use a generator to generate the test cases and/or libraries for the task, instead of writing them all by hand.\n\n\nWe add a generator like this:\n\n\n\n\nWe use the task made in the last example\n\n\nWe write a script \ngen.sh\n which generates the files upon execution; it must be a shell script, and it must write the files following the same tree as the \nfiles\n folder\n\n\nWe add the generator to the task with \ntaskstarter.py add generator tests/gen/gen.sh\n\n\nFinally, we test the task with \ntaskstarter.py test\n.\n\n\n\n\nThe generator is handy when a large number of test cases must be generated, and also for complex tasks where the expected output can take a long time to be computed, and thus needs to be precomputed.\n\n\nNote that the auto-test with \ntaskstarter.py test\n will show:\n\n\nTest successful on 2 correctSolutions with up to 4 test cases.\n\n\n\nwhich means the taskgrader successfully found all 4 test cases, the 2 test cases given directly in the task folder, and the 2 test cases generated by our script \ngen.sh\n.\n\n\nThis example can be found in the \nexamples/task4\n folder.\n\n\nTesting tasks\n\n\nTasks can be tested with:\n\n\n\n\ntaskstarter.py test\n, which will use the tool \ngenJson\n to prepare the task for usage (read about \ndefaultParams.json\n file below for more information) and test it for valid compilation, and test that the \"correct solutions\" get the expected grades.\n\n\ntaskstarter.py testsol [SOLUTION.c]\n, which if the task is valid, will test \nSOLUTION.c\n against the task. It is meant for quick solution testing; it uses the \nstdGrade\n tool.\n\n\n\n\nUsing tasks\n\n\nThe tool \ngenJson\n, automatically called when using \ntaskstarter.py test\n, prepares the task by writing its parameters into a \ndefaultParams.json\n file. It contains all the required information to evaluate solutions against the task, and can be used by evaluation platforms directly to reference the task. The tool \nstdGrade\n will use this file to quickly evaluate solutions.\n\n\nMore complex task writing\n\n\nMore complex tasks can be written for usage with the taskgrader. The \ntaskstarter\n tool is meant for simple tasks, you need to edit files manually for these examples. Here is an example, but read the rest of this documentation for more information.\n\n\nExample 5: solution skeleton\n\n\nSometimes, the \"solution\" to be evaluated is not the file to be executed, but a library or a test file.\n\n\nIn this example, we have \nrunner.py\n calling the function \nmin3nombres\n from the user-sent python file. The \"solution\" is hence a library, and the actual script executed is \nrunner.py\n using this library.\n\n\nIn order to be able to evaluation solutions against this task, we add, in \ntaskSettings.json\n, the key \noverrideParams/defaultSkeleton\n. This key will get copied directly to \ndefaultParams.json\n, where it will indicate the skeleton of what needs to be executed by the taskgrader.\n\n\nIn this key, we indicate what needs to be executed, and the values \n'%solname', '%solpath', '%sollang', '%soldeps'\n will be replaced with the solution's name, path, language and dependencies. In this example task, \nrunner.py\n is defined as the main program to execute, and our solution is passed as dependency of this program, and automatically named \nsolution.py\n to be then imported by \nrunner.py\n.\n\n\nYou can test the task by running, from the task folder:\n\n\n\n\ntaskstarter.py test\n for the normal test\n\n\ntaskstarter.py testsol tests/gen/sol-ok-py.py\n for a good solution\n\n\ntaskstarter.py testsol tests/gen/sol-bad-py.py\n for a bad solution\n\n\n\n\nThe testing tool \nstdGrade\n will automatically understand how to evaluate these solutions.\n\n\nThis example can be found in the \nexamples/task5\n folder.", 
            "title": "Writing tasks"
        }, 
        {
            "location": "/writing/#getting-started-on-writing-a-task", 
            "text": "Use  taskstarter.py init [taskpath]  to interactively create a new task.  A \"task\" is a set of programs and files representing the problem the solutions will be evaluated against:   the test cases (input files and the associated expected output),  the libraries the solutions can use  an optional generator which generates these two types of files  a sanitizer, checking the input files are in the required format  the checker, grading each solution's output   The script  tools/taskstarter/taskstarter.py  can assist with writing a task; use  taskstarter.py help  to see the available commands.  Here are some examples based around a simple problem: the program is given a number as input, and must output the double of the number. These example tasks can be found in the  examples  folder.", 
            "title": "Getting started on writing a task"
        }, 
        {
            "location": "/writing/#example-1-only-test-cases", 
            "text": "A task can be just test cases. The task can be built and tested like this:   We start our task in a folder with  taskstarter.py init  (answering 'no' to all questions); it will give us a base task structure we can use as reference  We put the test cases input files  test1.in  and  test2.in  in the subfolder  tests/files/  We put in the same subfolder  test1.out  and  test2.out , the correct output of these test cases  We can write a valid solution, for instance  sol-ok-c.c  in this example  We can test this solution with  taskstarter.py testsol tests/gen/sol-ok-c.c , it will say the solution got a grade of 100 for each test   When the task is only test cases, the tools will use default programs as sanitizer and checker. The solution will get a perfect grade of 100 if its output is the expected output ( test1.out  for the test case  test1.in , ...), or the minimum grade of 0 if its output is different.  This example can be found in the  examples/task1  folder.", 
            "title": "Example 1: only test cases"
        }, 
        {
            "location": "/writing/#adding-correct-solutions-to-our-task", 
            "text": "Once our task is almost ready, we can add \"correct solutions\": they are known solutions which have known results, such as always giving a good answer or always giving a bad answer.  It allows to test against regressions, that is to say that after modifications, that the task still gives the right grade to our known solutions.  Using again our example task 1, we can add correct solutions like this:  taskstarter.py addsol -g 100 -l c tests/gen/sol-ok-c.c  It means that each time we'll test the task, the solution  sol-ok-c.c  will be automatically evaluated against the task, and  -g 100  means we expect the solution to get a perfect grade of 100 each time.  We can also add an invalid solution, that is to say, a solution who will get the minimum grade of 0 each time. In this example, we have such a solution,  sol-bad-c.c . We add it as a \"bad solution\" like this:  taskstarter.py addsol -g 0 -l c tests/gen/sol-bad-c.c  which means that we expect it to get a grade of 0 on each test.  We can finally test the task by executing, in the task folder,  taskstarter.py test  in the task folder. It will output:  Test successful on 2 correctSolutions with up to 2 test cases.  which means the task successfully compiled, that the evaluation of the good solution  sol-ok-c.c  got a perfect grade of 100 each time, and that the evaluation of the bad solution  sol-bad-c.c  got a bad grade of 0 each time.", 
            "title": "Adding correct solutions to our task"
        }, 
        {
            "location": "/writing/#example-2-adding-a-sanitizer-and-a-checker", 
            "text": "It's generally recommended to use a sanitizer and a checker: the sanitizer will ensure only valid input test files are given to solutions, especially in cases where contestants are allowed to use their own test files as examples; the checker can give a more precise grade to solutions, and also handle cases where the solution output is not in the exact format expected.  We add a sanitizer and a checker to our test like this:   We use the task made in the last example  We write a script  sanitizer.sh  which takes as input the test case, and sets its exit code to 0 if it's valid, 1 if it's not  We write a script  checker.sh  which takes three arguments,  test.in  the test case input,  test.solout  the solution output,  test.out  the reference (expected) output; and gives a grade to the solution based on its output  We add the sanitizer and the checker to the task with  taskstarter.py add checker tests/gen/checker.sh  and  taskstarter.py add sanitizer tests/gen/sanitizer.sh  Finally, we test the task with  taskstarter.py test . It will tell us whether our programs were detected, compiled and executed successfully, and whether the \"correct solution\" we defined passed the test and got the expected grade of 100.   The sanitizer and the checker can be written in any language supported by the taskgrader.  This example can be found in the  examples/task2  folder.", 
            "title": "Example 2: adding a sanitizer and a checker"
        }, 
        {
            "location": "/writing/#example-3-adding-libraries", 
            "text": "This example is a task with libraries intended for usage by solutions. It's pretty simple to add libraries:   We write a library  lib.h  intended for usage with solution in the C language  We put it in  tests/files/lib/c/lib.h  Finally, we test the task with  taskstarter.py test .   As long as the libraries are in the right folder (by default,  tests/files/lib/[language]/[library.ext] ), they will be automatically detected by  genJson  and added to the default dependencies for that language. All solutions of that language will have the library available for importation without any configuration necessry.  This example can be found in the  examples/task3  folder.", 
            "title": "Example 3: adding libraries"
        }, 
        {
            "location": "/writing/#example-4-adding-a-generator", 
            "text": "It can be handy to use a generator to generate the test cases and/or libraries for the task, instead of writing them all by hand.  We add a generator like this:   We use the task made in the last example  We write a script  gen.sh  which generates the files upon execution; it must be a shell script, and it must write the files following the same tree as the  files  folder  We add the generator to the task with  taskstarter.py add generator tests/gen/gen.sh  Finally, we test the task with  taskstarter.py test .   The generator is handy when a large number of test cases must be generated, and also for complex tasks where the expected output can take a long time to be computed, and thus needs to be precomputed.  Note that the auto-test with  taskstarter.py test  will show:  Test successful on 2 correctSolutions with up to 4 test cases.  which means the taskgrader successfully found all 4 test cases, the 2 test cases given directly in the task folder, and the 2 test cases generated by our script  gen.sh .  This example can be found in the  examples/task4  folder.", 
            "title": "Example 4: adding a generator"
        }, 
        {
            "location": "/writing/#testing-tasks", 
            "text": "Tasks can be tested with:   taskstarter.py test , which will use the tool  genJson  to prepare the task for usage (read about  defaultParams.json  file below for more information) and test it for valid compilation, and test that the \"correct solutions\" get the expected grades.  taskstarter.py testsol [SOLUTION.c] , which if the task is valid, will test  SOLUTION.c  against the task. It is meant for quick solution testing; it uses the  stdGrade  tool.", 
            "title": "Testing tasks"
        }, 
        {
            "location": "/writing/#using-tasks", 
            "text": "The tool  genJson , automatically called when using  taskstarter.py test , prepares the task by writing its parameters into a  defaultParams.json  file. It contains all the required information to evaluate solutions against the task, and can be used by evaluation platforms directly to reference the task. The tool  stdGrade  will use this file to quickly evaluate solutions.", 
            "title": "Using tasks"
        }, 
        {
            "location": "/writing/#more-complex-task-writing", 
            "text": "More complex tasks can be written for usage with the taskgrader. The  taskstarter  tool is meant for simple tasks, you need to edit files manually for these examples. Here is an example, but read the rest of this documentation for more information.", 
            "title": "More complex task writing"
        }, 
        {
            "location": "/writing/#example-5-solution-skeleton", 
            "text": "Sometimes, the \"solution\" to be evaluated is not the file to be executed, but a library or a test file.  In this example, we have  runner.py  calling the function  min3nombres  from the user-sent python file. The \"solution\" is hence a library, and the actual script executed is  runner.py  using this library.  In order to be able to evaluation solutions against this task, we add, in  taskSettings.json , the key  overrideParams/defaultSkeleton . This key will get copied directly to  defaultParams.json , where it will indicate the skeleton of what needs to be executed by the taskgrader.  In this key, we indicate what needs to be executed, and the values  '%solname', '%solpath', '%sollang', '%soldeps'  will be replaced with the solution's name, path, language and dependencies. In this example task,  runner.py  is defined as the main program to execute, and our solution is passed as dependency of this program, and automatically named  solution.py  to be then imported by  runner.py .  You can test the task by running, from the task folder:   taskstarter.py test  for the normal test  taskstarter.py testsol tests/gen/sol-ok-py.py  for a good solution  taskstarter.py testsol tests/gen/sol-bad-py.py  for a bad solution   The testing tool  stdGrade  will automatically understand how to evaluate these solutions.  This example can be found in the  examples/task5  folder.", 
            "title": "Example 5: solution skeleton"
        }, 
        {
            "location": "/errors/", 
            "text": "Error messages\n\n\nIsolate is not properly installed\n\n\nThis error message happens when \nisolate\n, the tool used to isolate solution executions and gather metrics, was not properly installed. The taskgrader will fall back to a normal execution, which means the execution will not be isolated (allowing the solution to access the whole filesystem, or communicate over the network, for instance), and the taskgrader will not be able to tell how much time and memory the execution used. It's okay for a test environment, but \nisolate\n needs to be configured properly for a contest environment.\n\n\nThe script \ninstall.sh\n normally takes care of installing \nisolate\n properly; if not, try launching it again and looking for any error message related to \nisolate\n.\n\n\nUnable to import jsonschema\n\n\nThe taskgrader uses \njsonschema\n for input and output JSON validation. It should normally be downloaded by the \ninstall.sh\n script, but it may fail if \ngit\n is not installed. This validation is not mandatory, but if the input JSON is not valid, the taskgrader will most likely crash. The validation helps knowing which JSONs are invalid and why.\n\n\nIf \npip\n is available, you can install jsonschema automatically with \npip install jsonschema\n, alternatively you can download it manually from the \njsonschema GitHub repository\n.", 
            "title": "Error messages"
        }, 
        {
            "location": "/errors/#error-messages", 
            "text": "", 
            "title": "Error messages"
        }, 
        {
            "location": "/errors/#isolate-is-not-properly-installed", 
            "text": "This error message happens when  isolate , the tool used to isolate solution executions and gather metrics, was not properly installed. The taskgrader will fall back to a normal execution, which means the execution will not be isolated (allowing the solution to access the whole filesystem, or communicate over the network, for instance), and the taskgrader will not be able to tell how much time and memory the execution used. It's okay for a test environment, but  isolate  needs to be configured properly for a contest environment.  The script  install.sh  normally takes care of installing  isolate  properly; if not, try launching it again and looking for any error message related to  isolate .", 
            "title": "Isolate is not properly installed"
        }, 
        {
            "location": "/errors/#unable-to-import-jsonschema", 
            "text": "The taskgrader uses  jsonschema  for input and output JSON validation. It should normally be downloaded by the  install.sh  script, but it may fail if  git  is not installed. This validation is not mandatory, but if the input JSON is not valid, the taskgrader will most likely crash. The validation helps knowing which JSONs are invalid and why.  If  pip  is available, you can install jsonschema automatically with  pip install jsonschema , alternatively you can download it manually from the  jsonschema GitHub repository .", 
            "title": "Unable to import jsonschema"
        }, 
        {
            "location": "/moreinfo/", 
            "text": "How does it work?\n\n\nHere's a description of the evaluation process that taskgrader does, managed by the function \nevaluation(evaluationParams)\n.\n\n\n\n\nevaluationParams\n is the input JSON\n\n\nA build folder is created for the evaluation\n\n\nThe \ndefaultParams.json\n file from the task is read and its variables added\n\n\nA \ndictWithVars\n is created to add the variables, and returns the JSON data as if variables were replaced by their values\n\n\ngenerators\n are compiled\n\n\ngenerations\n describe how \ngenerators\n are to be executed in order to generate all the test files and optional libraries\n\n\nextraTests\n are added into the tests pool\n\n\nThe \nsanitizer\n and the \nchecker\n are compiled\n\n\nThe \nsolutions\n are compiled\n\n\nAll \nexecutions\n are done for the solutions\n\n\nThe full evaluation report is returned on standard output\n\n\n\n\nExecutions\n\n\nEach execution is the grading of one solution against multiple test files. For each \nexecution\n:\n\n Test files corresponding to \nfilterTests\n are selected, then for each test file:\n\n It passes first the \nsanitizer\n test\n\n Then the solution is executed, with the test file as standard input and the output saved\n\n Finally the \nchecker\n grades the solution according to its output on that particular test file\n\n\nfilterTests\n is a list of globs (as \n\"test*.in\"\n or \n\"mytest.in\"\n) selecting test files to use among all the test files generated by the generators, and the \nextraTests\n given. One can specify directly test files into this array to use only specific ones.\n\n\nEvaluation components\n\n\nThe evaluation is made against a task which has multiple components.\n\n\nGenerators\n\n\nThe \ngenerators\n are generating the testing environment. They are executed, optionally with various parameters, to generate files, which can be:\n\n\n\n\ntest files: inputs for the solution, and if necessary, expected output results\n\n\nlibraries, for the compilation and execution of solutions\n\n\n\n\nSome of these files can be passed directly in the evaluation JSON, without the need of a generator.\n\n\nSanitizer\n\n\nThe \nsanitizer\n checks whether a test input is valid. It expects the test input on its stdin, and its exit code indicates the validity of the data.\n\n\nChecker\n\n\nThe \nchecker\n checks whether the output of a solution corresponds to the expected result. It expects three arguments on the command line:\n\n\n\n\ntest.solout\n the solution output\n\n\ntest.in\n the reference input\n\n\ntest.out\n the reference output\n\n\n\n\nAll checkers are passed these three arguments, whether they use it or not. The checker outputs the grading of the solution; its exit code can indicate an error while checking (invalid arguments, missing files, ...).\n\n\nTools\n\n\nVarious tools are available in the subfolder \ntools\n. They can be configured with their respective \nconfig.py\n files.\n\n\nCreating a task\n\n\ntaskstarter.py\n helps task writers create and modify simple tasks. This simple tool is meant as a starting point for people not knowing how the taskgrader works but willing to write a task, and helps them through documented steps. It allows to do some operations in tasks folders, such as creating the base skeleton, giving some help on various components and testing the task. This tool creates a \ntaskSettings.json\n in the task folder, that \ngenJson.py\n can then use to create a \ndefaultParams.json\n accordingly. Read the \"Getting started on writing a task\" section for more information.\n\n\nPreparing a task for grading\n\n\ngenJson.py\n analyses tasks and creates the \ndefaultParams.json\n file for them. It will read the \ntaskSettings.json\n file in each task for some settings and try to automatically detect other settings.\n\n\ntaskSettings.json\n\n\nThe \ntaskSettings.json\n is JSON data giving some parameters about the task, for use by \ngenJson.py\n. It has the following keys:\n\n\n\n\ngenerator\n: path to the generator of the task\n\n\ngeneratorDeps\n: dependencies for the generator (list of fileDescr, see the input JSON schema for more information)\n\n\nsanitizer\n, \nsanitizerDeps\n, \nsanitizerLang\n: sanitizer of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)\n\n\nchecker\n, \ncheckerDeps\n, \ncheckerLang\n: checker of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)\n\n\nextraDir\n: folder with extra files (input test files and/or libraries)\n\n\noverrideParams\n: JSON data to be copied directly into \ndefaultParams.json\n, will replace any key with the same name from \ngenJson.py\n generated JSON data\n\n\ncorrectSolutions\n: list of solutions known as working with the task, will be tested by \ngenJson.py\n which will check whether they get the right results. Each solution must have the following keys: \npath\n, \nlang\n and \ngrade\n (the numerical grade the solution is supposed to get).\n\n\n\n\ndefaultParams.json\n\n\nThe \ndefaultParams.json\n is a task file giving some information about the task, must be JSON data pairing the following keys with the right objects:\n\n\n\n\nrootPath\n: the root path of the files\n\n\ndefaultGenerator\n: a default generator\n\n\ndefaultGeneration\n: the default generation for the default generator\n\n\nextraTests\n (optional): some extra tests\n\n\ndefaultSanitizer\n: the default sanitizer\n\n\ndefaultChecker\n: the default checker\n\n\ndefaultDependencies-[language]\n (optional): default dependencies for that language; if not defined, it will fallback to \ndefaultDependencies\n or to an empty list\n\n\ndefaultFilterTests-[language]\n (optional): default glob-style filters for the tests for that language; if not defined, it will fallback to \ndefaultFilterTests\n or to an empty list\n\n\n\n\nGrading a solution\n\n\nstdGrade.sh\n allows to easily grade a solution. The task path must be the current directory, or must be specified with \n-p\n. It will expect to have a \ndefaultParams.json\n file in the task directory, describing the task with some variables. Note that it's meant for fast and simple grading of solutions, it doesn't give a full control over the evaluation process. \nstdGrade.sh\n is a shortcut to two utilities present in its folder, for more options, see \ngenStdTaskJson.py -h\n.\n\n\nBasic usage: \nstdGrade.sh [SOLUTION]...\n from a task folder.\n\n\nExit codes\n\n\nThe taskgrader will return the following exit codes:\n\n\n\n\n0\n if the evaluation took place without error\n\n\n1\n if an error with the evaluation happened, usually because of the evaluation parameters themselves\n\n\n2\n if there was a temporary error, meaning the same evaluation should be tried again at a later time\n\n\n3\n if the evaluation needed a language which is not supported, or which lacks a dependency to compile\n\n\n\n\nInternals (for developers)\n\n\nevaluation\n is the evaluation process. It reads an input JSON and preprocesses it to replace the variables.\n\n\nEach program is defined as an instance of the class Program, that we \ncompile\n, then \nprepareExecution\n to set the execution parameters, then \nexecute\n with the proper parameters.\n\n\nLanguages are set as classes which define two functions: \ngetSource\n which defines how to search for some dependencies for this language, and \ncompile\n which is the compilation process.\n\n\nThe cache is handled by various Cache classes, each storing the cache parameters for a specific program and giving access to the various cache folders corresponding to compilation or execution of said programs.", 
            "title": "Further information"
        }, 
        {
            "location": "/moreinfo/#how-does-it-work", 
            "text": "Here's a description of the evaluation process that taskgrader does, managed by the function  evaluation(evaluationParams) .   evaluationParams  is the input JSON  A build folder is created for the evaluation  The  defaultParams.json  file from the task is read and its variables added  A  dictWithVars  is created to add the variables, and returns the JSON data as if variables were replaced by their values  generators  are compiled  generations  describe how  generators  are to be executed in order to generate all the test files and optional libraries  extraTests  are added into the tests pool  The  sanitizer  and the  checker  are compiled  The  solutions  are compiled  All  executions  are done for the solutions  The full evaluation report is returned on standard output", 
            "title": "How does it work?"
        }, 
        {
            "location": "/moreinfo/#executions", 
            "text": "Each execution is the grading of one solution against multiple test files. For each  execution :  Test files corresponding to  filterTests  are selected, then for each test file:  It passes first the  sanitizer  test  Then the solution is executed, with the test file as standard input and the output saved  Finally the  checker  grades the solution according to its output on that particular test file  filterTests  is a list of globs (as  \"test*.in\"  or  \"mytest.in\" ) selecting test files to use among all the test files generated by the generators, and the  extraTests  given. One can specify directly test files into this array to use only specific ones.", 
            "title": "Executions"
        }, 
        {
            "location": "/moreinfo/#evaluation-components", 
            "text": "The evaluation is made against a task which has multiple components.", 
            "title": "Evaluation components"
        }, 
        {
            "location": "/moreinfo/#generators", 
            "text": "The  generators  are generating the testing environment. They are executed, optionally with various parameters, to generate files, which can be:   test files: inputs for the solution, and if necessary, expected output results  libraries, for the compilation and execution of solutions   Some of these files can be passed directly in the evaluation JSON, without the need of a generator.", 
            "title": "Generators"
        }, 
        {
            "location": "/moreinfo/#sanitizer", 
            "text": "The  sanitizer  checks whether a test input is valid. It expects the test input on its stdin, and its exit code indicates the validity of the data.", 
            "title": "Sanitizer"
        }, 
        {
            "location": "/moreinfo/#checker", 
            "text": "The  checker  checks whether the output of a solution corresponds to the expected result. It expects three arguments on the command line:   test.solout  the solution output  test.in  the reference input  test.out  the reference output   All checkers are passed these three arguments, whether they use it or not. The checker outputs the grading of the solution; its exit code can indicate an error while checking (invalid arguments, missing files, ...).", 
            "title": "Checker"
        }, 
        {
            "location": "/moreinfo/#tools", 
            "text": "Various tools are available in the subfolder  tools . They can be configured with their respective  config.py  files.", 
            "title": "Tools"
        }, 
        {
            "location": "/moreinfo/#creating-a-task", 
            "text": "taskstarter.py  helps task writers create and modify simple tasks. This simple tool is meant as a starting point for people not knowing how the taskgrader works but willing to write a task, and helps them through documented steps. It allows to do some operations in tasks folders, such as creating the base skeleton, giving some help on various components and testing the task. This tool creates a  taskSettings.json  in the task folder, that  genJson.py  can then use to create a  defaultParams.json  accordingly. Read the \"Getting started on writing a task\" section for more information.", 
            "title": "Creating a task"
        }, 
        {
            "location": "/moreinfo/#preparing-a-task-for-grading", 
            "text": "genJson.py  analyses tasks and creates the  defaultParams.json  file for them. It will read the  taskSettings.json  file in each task for some settings and try to automatically detect other settings.", 
            "title": "Preparing a task for grading"
        }, 
        {
            "location": "/moreinfo/#tasksettingsjson", 
            "text": "The  taskSettings.json  is JSON data giving some parameters about the task, for use by  genJson.py . It has the following keys:   generator : path to the generator of the task  generatorDeps : dependencies for the generator (list of fileDescr, see the input JSON schema for more information)  sanitizer ,  sanitizerDeps ,  sanitizerLang : sanitizer of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)  checker ,  checkerDeps ,  checkerLang : checker of the task (path, dependencies, language; default is no dependencies and auto-detect language depending on extension)  extraDir : folder with extra files (input test files and/or libraries)  overrideParams : JSON data to be copied directly into  defaultParams.json , will replace any key with the same name from  genJson.py  generated JSON data  correctSolutions : list of solutions known as working with the task, will be tested by  genJson.py  which will check whether they get the right results. Each solution must have the following keys:  path ,  lang  and  grade  (the numerical grade the solution is supposed to get).", 
            "title": "taskSettings.json"
        }, 
        {
            "location": "/moreinfo/#defaultparamsjson", 
            "text": "The  defaultParams.json  is a task file giving some information about the task, must be JSON data pairing the following keys with the right objects:   rootPath : the root path of the files  defaultGenerator : a default generator  defaultGeneration : the default generation for the default generator  extraTests  (optional): some extra tests  defaultSanitizer : the default sanitizer  defaultChecker : the default checker  defaultDependencies-[language]  (optional): default dependencies for that language; if not defined, it will fallback to  defaultDependencies  or to an empty list  defaultFilterTests-[language]  (optional): default glob-style filters for the tests for that language; if not defined, it will fallback to  defaultFilterTests  or to an empty list", 
            "title": "defaultParams.json"
        }, 
        {
            "location": "/moreinfo/#grading-a-solution", 
            "text": "stdGrade.sh  allows to easily grade a solution. The task path must be the current directory, or must be specified with  -p . It will expect to have a  defaultParams.json  file in the task directory, describing the task with some variables. Note that it's meant for fast and simple grading of solutions, it doesn't give a full control over the evaluation process.  stdGrade.sh  is a shortcut to two utilities present in its folder, for more options, see  genStdTaskJson.py -h .  Basic usage:  stdGrade.sh [SOLUTION]...  from a task folder.", 
            "title": "Grading a solution"
        }, 
        {
            "location": "/moreinfo/#exit-codes", 
            "text": "The taskgrader will return the following exit codes:   0  if the evaluation took place without error  1  if an error with the evaluation happened, usually because of the evaluation parameters themselves  2  if there was a temporary error, meaning the same evaluation should be tried again at a later time  3  if the evaluation needed a language which is not supported, or which lacks a dependency to compile", 
            "title": "Exit codes"
        }, 
        {
            "location": "/moreinfo/#internals-for-developers", 
            "text": "evaluation  is the evaluation process. It reads an input JSON and preprocesses it to replace the variables.  Each program is defined as an instance of the class Program, that we  compile , then  prepareExecution  to set the execution parameters, then  execute  with the proper parameters.  Languages are set as classes which define two functions:  getSource  which defines how to search for some dependencies for this language, and  compile  which is the compilation process.  The cache is handled by various Cache classes, each storing the cache parameters for a specific program and giving access to the various cache folders corresponding to compilation or execution of said programs.", 
            "title": "Internals (for developers)"
        }
    ]
}